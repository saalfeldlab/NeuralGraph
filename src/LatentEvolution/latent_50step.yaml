# Default configuration for 5-step Latent Space Model
# Activity data is observed every 5 time steps; stimulus available at every step

latent_dims: 256 # Dimensionality of latent space
num_neurons: 13741 # Number of observed neurons (input/output size)
use_batch_norm: False
# Activation function from torch.nn (e.g., ReLU, GELU, Tanh, Mish)
activation: ReLU

encoder_params:
  num_hidden_layers: 1 # Layers in the encoder MLP
  num_hidden_units: 256 # Hidden units per layer
  l1_reg_loss: 0.0
  use_input_skips: true # Use MLPWithSkips (input fed to each hidden layer via concatenation)

decoder_params:
  num_hidden_layers: 1 # Layers in the decoder MLP
  num_hidden_units: 256 # Hidden units per layer
  l1_reg_loss: 0.0
  use_input_skips: true # Use MLPWithSkips (input fed to each hidden layer via concatenation)

evolver_params:
  num_hidden_layers: 1 # Hidden layers in the evolver network
  num_hidden_units: 256 # Hidden units per layer in the evolver
  l1_reg_loss: 0.0
  activation: Tanh
  use_input_skips: true # Use MLPWithSkips (input fed to each hidden layer via concatenation)
  zero_init: true # Zero-initialize final layer so evolver starts as identity (z_{t+1} = z_t)
  tv_reg_loss: 0.0 # Total variation regularization on evolver updates (typical: 1e-5 to 1e-3)

stimulus_encoder_params:
  num_input_dims: 1736
  num_hidden_layers: 3
  num_hidden_units: 64
  num_output_dims: 64
  use_input_skips: true # Use MLPWithSkips (input fed to each hidden layer via concatenation)

training:
  time_units: 50
  # Acquisition mode: time_aligned = observations only at 0, tu, 2tu, ...
  # (all neurons observed simultaneously at aligned time points)
  acquisition_mode:
    mode: time_aligned
  # Number of time_units multiples to evolve. Loss applied at each multiple.
  # E.g., evolve_multiple_steps=2 with time_units=5 applies loss at t+5 and t+10
  evolve_multiple_steps: 5
  simulation_config: "fly_N9_62_1_youtube-vos_calcium" # 987736 time steps
  data_passes_per_epoch: 1
  epochs: 30 # Total number of epochs
  save_best_checkpoint: True # Save model with lowest validation loss
  save_checkpoint_every_n_epochs: 0
  diagnostics_freq_epochs: 1 # run validation diagnostics every epoch
  batch_size: 256 # Samples per batch
  learning_rate: 0.00001 # Learning rate for optimizer
  optimizer: Adam # Optimizer name from torch.optim
  train_step: train_step
  column_to_model: "VOLTAGE" # Options: CALCIUM, VOLTAGE, FLUORESCENCE, STIMULUS
  use_tf32_matmul: true
  seed: 234
  # Loss function from torch.nn.functional (e.g., mse_loss, huber_loss, l1_loss)
  loss_function: mse_loss
  grad_clip_max_norm: 0.0
  # stimulus autoencoder pretraining: pretrains stimulus encoder/decoder for
  # reconstruction, copies encoder weights into main model, then freezes encoder
  pretrain_stimulus_ae: true
  stimulus_ae:
    epochs: 10
    batch_size: 256
    learning_rate: 0.0001
    optimizer: Adam

  data_split:
    train_start: 5120 # = 256*20 Exclude burn-in
    train_end: 972800 # = 256*3800
    validation_start: 973056 # = 256*3801
    validation_end: 987648 # 256*3858

# Cross-dataset validation (evaluates trained model on different datasets)
cross_validation_configs:
  - simulation_config: "fly_N9_62_0_optical_flow_calcium"
    name: "optical_flow"
    data_split:
      train_start: 0
      train_end: 1
      validation_start: 5000
      validation_end: 15000
  - simulation_config: "fly_N9_62_0_davis_calcium"
    name: "davis_2016_2017"
    data_split:
      train_start: 0
      train_end: 1
      validation_start: 5000
      validation_end: 25000
