# Staggered acquisition

Realistically data acquisition will be temporally staggered as shown below:

```
  staggered_random mode (tu=5, different phases per neuron)
  ==========================================================
  Each neuron observed at different phase + k*tu

  Time:     0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
           ┌──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┬──┐
  Neuron 1 │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  phase=0
  Neuron 2 │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │  phase=1
  Neuron 3 │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  phase=3
  Neuron 4 │  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  phase=4
  Neuron 5 │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  │ X│  │  │  │  phase=2
           └──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┴──┘
           ↑  ↑  ↑  ↑  ↑
        phases 0-4 spread across first tu timesteps
        (then repeat every tu steps)

```

Let's put aside the caveat that we will also be measuring Calcium and not voltage
directly, which we will turn to once we figure this one out.

## Initial experiment

Let's just try the latent model for the time-aligned case since we have that working.

```bash
bsub -J stag -q gpu_a100 -gpu "num=1" -n 8 -o acq_stag.log \
    python src/LatentEvolution/latent.py test_acq latent_20step.yaml \
    training.acquisition-mode:staggered-random-mode \
    --training.acquisition-mode.seed 42
```

This experiment totally fails. Note that the time-aligned experiment for the
same `time_units=20` works. Our model bakes in the assumption that the voltages
are generated by a system of _first order_ differential equations. When the data
is sheared in time this is no longer the case and the model fails to capture the
dynamics.

## Time aligned memory bank

TODO: fill this out.

## Add encoder-decoder warm up

It's possible that the previous experiment failed due to bad initialization. We are
randomly sampling the z0's and trying to decode & evolve over 100 steps and applying
a loss. Let's add in an encoder and do an intial training of the encoder/decoder
pretending that the staggered data are time-aligned. This could help us start off
the z0 memory bank & the decoder in the right ball park so that the training
converges to the desired 1-step operator.
