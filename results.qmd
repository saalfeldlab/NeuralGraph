---
title: "Results"
subtitle: "Signal Landscape Exploration Findings"
---

## Experiment Overview

The signal_landscape_Claude experiment systematically explored GNN training across 14 simulation regimes over 107 iterations.

::: {.callout-note}
## Objective

Map the landscape of GNN training configurations to understand:

- Which simulation regimes are learnable?
- What training parameters are critical?
- Are there fundamental limits to connectivity recovery?
:::

## Regime Summary

### Performance by Regime

| Block | Regime | eff_rank | Convergence | Best R² | Key Finding |
|-------|--------|----------|-------------|---------|-------------|
| 1 | Chaotic | ~34 | **100%** | 0.9999 | "Easy mode" - robust to 100x param variation |
| 2 | Low-rank | ~11 | 25% | 0.953 | Requires lr boost (1E-4 → 1E-3) |
| 3 | Dale's Law | ~30 | **100%** | 0.9998 | E/I constraint doesn't add difficulty |
| 4 | n_types=2 | ~34 | 37.5% | 0.987 | Dual-objective conflict (W vs embedding) |
| 5 | Noise | ~83 | **100%** | 0.9996 | Noise increases eff_rank |
| 6 | Low-rank + n_types | ~11 | 37.5% | 0.912 | Combined difficulty |
| 7 | Sparse (ff=0.2) | ~6 | **0%** | 0.08 | Fundamentally unrecoverable |
| 8 | Sparse + Noise | ~92 | 0% partial | 0.20 | Noise rescues rank but masks W |
| 9 | Intermediate (ff=0.5) | ~26 | 50% | 0.65 | W learning challenging |
| 10 | High fill (ff=0.75) | ~38 | **100%** | 0.985 | Easy transition from ff=0.5 |
| 11 | Scale n=200 | ~34 | 75% | 0.982 | lr_W tolerance narrower |
| 12 | Very high (ff=0.9) | ~45 | **100%** | 0.992 | Near-full connectivity |
| 13 | Scale n=300 | ~34 | 75% | 0.978 | Further scale sensitivity |
| 14 | Scale n=500 | ~34 | 50% | 0.965 | Upper boundary found at lr_W~8E-2 |

### Visual Summary

![Regime difficulty vs recovery quadrant chart showing 14 simulation regimes positioned by effective rank (x-axis) and connectivity recovery R² (y-axis). Green quadrant (Easy Mode) contains high-rank regimes like Chaotic, Dale, and Noise. Red quadrant (Unrecoverable) contains sparse regimes with low effective rank.](assets/landscape_quadrant.png){.lightbox}

## Key Discoveries

### 1. Effective Rank Determines Difficulty

::: {.metric-box}
::: {.value}
95%
:::
::: {.label}
Confidence in eff_rank as primary predictor
:::
:::

The effective rank (SVD rank at 99% variance) is the strongest predictor of training difficulty:

| eff_rank Range | Convergence | Interpretation |
|----------------|-------------|----------------|
| > 30 | 100% | Easy - any reasonable parameters work |
| 15-30 | Variable | Depends on other factors |
| 9-15 | 25-40% | Hard - requires careful tuning |
| < 8 | 0% | Unrecoverable - fundamental limit |

### 2. "Easy Mode" Regimes

Three regimes achieved 100% convergence:

::: {.panel-tabset}

#### Chaotic

- **eff_rank**: ~34
- **Tolerance**: 5x lr_W range, 100x L1 range
- **Best config**: lr_W=5E-3, L1=1E-4

#### Dale's Law

- **eff_rank**: ~30
- **Tolerance**: 100x lr_W range (5E-4 to 5E-2)
- **Finding**: E/I constraint doesn't add difficulty

#### Noise

- **eff_rank**: ~83 (noise increases rank!)
- **Tolerance**: Very robust
- **Finding**: "Super easy mode"

:::

### 3. Low-Rank Challenge

Low-rank connectivity (eff_rank ~11) requires specific tuning:

```
Block 2 Progress:
Iter 9:  lr_W=5E-3, lr=1E-4  → R²=0.355 (failed)
Iter 10: factorization=True  → R²=0.355 (no help)
Iter 13: lr_W=8E-3           → R²=0.351 (failed)
Iter 15: lr=5E-4             → R²=0.912 (partial!)
Iter 16: lr=1E-3             → R²=0.953 (BREAKTHROUGH)
```

::: {.callout-tip}
## Key Insight

For low-rank regimes, the **MLP learning rate** (not lr_W) is the critical parameter. Boosting from 1E-4 to 1E-3 enabled breakthrough.
:::

### 4. Dual-Objective Conflict

With multiple neuron types (n_types > 1), two objectives compete:

```{mermaid}
graph LR
    A[High lr_W] --> B[Fast W learning]
    A --> C[Starved embedding]
    C --> D[Poor cluster_acc]

    E[Balanced lr_W] --> F[Slower W learning]
    E --> G[Good embedding]
    G --> H[High cluster_acc]

    style C fill:#ffcccc
    style D fill:#ffcccc
    style G fill:#ccffcc
    style H fill:#ccffcc
```

**Solution**: Use `lr_embedding_start` separately to compensate:

| Config | lr_W | lr_emb | R² | cluster_acc |
|--------|------|--------|-----|-------------|
| Baseline | 5E-3 | 1E-4 | 0.85 | 0.48 |
| Fixed | 5E-3 | 1E-3 | 0.92 | 0.95 |

### 5. Sparse Unrecoverability

::: {.callout-warning}
## Fundamental Limit Discovered

Sparse connectivity (ff=0.2) is **fundamentally unrecoverable**, regardless of training parameters.
:::

Evidence from 16 failed attempts:

| Intervention | Result |
|--------------|--------|
| lr_W sweep (1E-4 to 5E-2) | All failed |
| L1 sweep (1E-6 to 1E-3) | All failed |
| 5x data augmentation | No improvement |
| Noise addition | eff_rank rescued (6→92) but R²=0.20 plateau |

**Root cause**: With ff=0.2 and n=100, the connectivity matrix W has only ~200 non-zero entries. The identifiability ceiling is R² ≈ ff (linear law).

### 6. Noise Effects

Noise has a dual effect:

| Effect | Mechanism | Impact |
|--------|-----------|--------|
| **Positive** | Increases eff_rank | Makes training easier |
| **Negative** | Masks W signal | Limits max R² |

In sparse regime:

- Without noise: eff_rank=6, R²<0.1 (untrainable)
- With noise: eff_rank=92, R²~0.20 (trainable but limited)

## Training Parameter Insights

### Learning Rate Landscape

```
                    Low eff_rank (<15)    High eff_rank (>30)
                    ─────────────────     ────────────────────
lr_W optimal:       5E-3 to 1E-2         2E-3 to 5E-2
lr_W range:         ~2x                   ~25x (very tolerant)

lr optimal:         1E-3 (critical!)      1E-4 to 1E-3
lr range:           narrow                wide

L1 optimal:         1E-5 to 1E-4         1E-6 to 1E-3
L1 range:           ~10x                  ~1000x
```

### Parameter Sensitivity

| Parameter | Low eff_rank | High eff_rank |
|-----------|--------------|---------------|
| lr_W | Moderate | Low (very tolerant) |
| lr | **Critical** | Low |
| L1 | High | Low |
| data_augmentation | Moderate | Low |

## Scaling Experiments

### n_neurons Scaling (Blocks 13-14)

| n_neurons | lr_W Range | Convergence | Notes |
|-----------|------------|-------------|-------|
| 100 | 40x (1E-4 to 4E-3) | Baseline | Well-explored |
| 200 | 20x (1E-3 to 2E-2) | Good | Narrower than 100 |
| 300 | 10x (5E-3 to 5E-2) | Good | Even narrower |
| 500 | Testing | In progress | Boundary at ~8E-2 |

::: {.callout-note}
## Scaling Law

As network size increases, the lr_W tolerance window narrows approximately as 1/√n.
:::

## Summary Statistics

### Overall Performance

::: {.feature-grid}

::: {.feature-card}
### 107 Iterations
Across 14 simulation regimes
:::

::: {.feature-card}
### 12 Principles
Novel findings with >75% confidence
:::

::: {.feature-card}
### 6 Easy Regimes
100% convergence (chaotic, Dale, noise, ff=0.75, ff=0.9)
:::

::: {.feature-card}
### 1 Hard Limit
ff<0.3 fundamentally unrecoverable
:::

:::

### Metric Distributions

| Metric | Min | Max | Mean | Std |
|--------|-----|-----|------|-----|
| connectivity_R² | 0.02 | 0.9999 | 0.71 | 0.35 |
| test_R² | 0.15 | 0.99 | 0.82 | 0.18 |
| effective_rank | 6 | 92 | 31 | 24 |
| cluster_accuracy | 0.25 | 1.0 | 0.78 | 0.22 |

## Conclusions

1. **eff_rank is primary**: Effective rank determines training difficulty more than any single parameter

2. **Easy modes exist**: Chaotic, Dale, noise, ff=0.75, and ff=0.9 regimes are robust to wide parameter ranges

3. **Low-rank needs lr boost**: MLP learning rate (not lr_W) is critical for low-rank recovery

4. **Fundamental limits**: Sparse connectivity (ff<0.3) has an identifiability ceiling

5. **Noise is double-edged**: Increases eff_rank but masks connectivity signal

6. **Scale matters**: Larger networks require tighter parameter tuning - lr_W tolerance scales as ~1/√n

7. **Fill factor gradient**: Connectivity recovery improves smoothly from ff=0.5 through ff=0.9

## Next Steps

- [Epistemic Analysis](epistemic-analysis.qmd) - How these findings were discovered
- [GNN Model](gnn-model.qmd) - Architecture details
- [Experiment Loop](experiment-loop.qmd) - Methodology
