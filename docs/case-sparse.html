<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Case Study: Sparse Connectivity – NeuralGraph: LLM-in-the-Loop</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">NeuralGraph: LLM-in-the-Loop</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-system" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">System</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-system">    
        <li>
    <a class="dropdown-item" href="./architecture.html">
 <span class="dropdown-text">Architecture</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./experiment-loop.html">
 <span class="dropdown-text">Experiment Loop</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-analysis" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Analysis</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-analysis">    
        <li>
    <a class="dropdown-item" href="./epistemic-analysis.html">
 <span class="dropdown-text">Epistemic Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./results.html">
 <span class="dropdown-text">Results</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./exploration-gallery.html">
 <span class="dropdown-text">Exploration Gallery</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-case-studies" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Case Studies</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-case-studies">    
        <li>
    <a class="dropdown-item" href="./case-low-rank.html">
 <span class="dropdown-text">Low-Rank Connectivity</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./case-sparse.html">
 <span class="dropdown-text">Sparse Connectivity</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./gnn-model.html"> 
<span class="menu-text">GNN Model</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/allierc/NeuralGraph"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link active" data-scroll-target="#problem-statement">Problem Statement</a></li>
  <li><a href="#why-code-changes" id="toc-why-code-changes" class="nav-link" data-scroll-target="#why-code-changes">Why Code Changes?</a></li>
  <li><a href="#config-baseline-iterations-18" id="toc-config-baseline-iterations-18" class="nav-link" data-scroll-target="#config-baseline-iterations-18">Config Baseline (Iterations 1–8)</a></li>
  <li><a href="#code-modification-1-w-initialization-scale" id="toc-code-modification-1-w-initialization-scale" class="nav-link" data-scroll-target="#code-modification-1-w-initialization-scale">Code Modification 1: W Initialization Scale</a></li>
  <li><a href="#code-modification-2-proximal-l1-soft-thresholding" id="toc-code-modification-2-proximal-l1-soft-thresholding" class="nav-link" data-scroll-target="#code-modification-2-proximal-l1-soft-thresholding">Code Modification 2: Proximal L1 Soft-Thresholding</a></li>
  <li><a href="#code-modification-3-mlp-capacity-reduction" id="toc-code-modification-3-mlp-capacity-reduction" class="nav-link" data-scroll-target="#code-modification-3-mlp-capacity-reduction">Code Modification 3: MLP Capacity Reduction</a></li>
  <li><a href="#code-modification-4-gradient-clipping-on-w" id="toc-code-modification-4-gradient-clipping-on-w" class="nav-link" data-scroll-target="#code-modification-4-gradient-clipping-on-w">Code Modification 4: Gradient Clipping on W</a></li>
  <li><a href="#code-modification-5-lin_phi-scaling-removal" id="toc-code-modification-5-lin_phi-scaling-removal" class="nav-link" data-scroll-target="#code-modification-5-lin_phi-scaling-removal">Code Modification 5: lin_phi Scaling / Removal</a></li>
  <li><a href="#code-modification-6-recurrent-training" id="toc-code-modification-6-recurrent-training" class="nav-link" data-scroll-target="#code-modification-6-recurrent-training">Code Modification 6: Recurrent Training</a></li>
  <li><a href="#code-modification-7-anti-sparsity-penalty-on-w" id="toc-code-modification-7-anti-sparsity-penalty-on-w" class="nav-link" data-scroll-target="#code-modification-7-anti-sparsity-penalty-on-w">Code Modification 7: Anti-Sparsity Penalty on W</a></li>
  <li><a href="#code-modification-8-freeze-lin_edge-after-warm-up" id="toc-code-modification-8-freeze-lin_edge-after-warm-up" class="nav-link" data-scroll-target="#code-modification-8-freeze-lin_edge-after-warm-up">Code Modification 8: Freeze lin_edge After Warm-Up</a></li>
  <li><a href="#code-modification-9-lin_edge-dropout" id="toc-code-modification-9-lin_edge-dropout" class="nav-link" data-scroll-target="#code-modification-9-lin_edge-dropout">Code Modification 9: lin_edge Dropout</a></li>
  <li><a href="#code-modification-10-lin_edge-mode-bypass-paradigm-shift" id="toc-code-modification-10-lin_edge-mode-bypass-paradigm-shift" class="nav-link" data-scroll-target="#code-modification-10-lin_edge-mode-bypass-paradigm-shift">Code Modification 10: lin_edge Mode Bypass (Paradigm Shift)</a></li>
  <li><a href="#seed-universality-confirmation" id="toc-seed-universality-confirmation" class="nav-link" data-scroll-target="#seed-universality-confirmation">Seed Universality Confirmation</a></li>
  <li><a href="#summary-what-failed-and-why" id="toc-summary-what-failed-and-why" class="nav-link" data-scroll-target="#summary-what-failed-and-why">Summary: What Failed and Why</a></li>
  <li><a href="#revised-architectural-diagnosis" id="toc-revised-architectural-diagnosis" class="nav-link" data-scroll-target="#revised-architectural-diagnosis">Revised Architectural Diagnosis</a></li>
  <li><a href="#ucb-exploration-trees" id="toc-ucb-exploration-trees" class="nav-link" data-scroll-target="#ucb-exploration-trees">UCB Exploration Trees</a></li>
  <li><a href="#next-steps-sgd-optimizer-for-w" id="toc-next-steps-sgd-optimizer-for-w" class="nav-link" data-scroll-target="#next-steps-sgd-optimizer-for-w">Next Steps: SGD Optimizer for W</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Case Study: Sparse Connectivity</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Breaking the 0.489 ceiling — LLM-guided code modifications to the GNN architecture (52 iterations)</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="problem-statement" class="level2">
<h2 class="anchored" data-anchor-id="problem-statement">Problem Statement</h2>
<p>Sparse connectivity (filling_factor=50%, n=100 neurons) produces neural activity with subcritical spectral radius (rho=0.746). The landscape exploration (blocks 7, 8, 17) established that <strong>no configuration sweep can break the conn_R<sup>2</sup>=0.489 ceiling</strong> — neither regularization, learning rates, epochs, noise, nor tripling the data (30k frames) had any effect. This motivated a dedicated exploration focused on <strong>modifying the GNN code itself</strong>.</p>
<p>The central question: <strong>can code-level modifications to the GNN architecture break the 0.489 ceiling?</strong> After 52 iterations testing 10 code modifications across 3 random seeds, the answer is <strong>no</strong> — the ceiling is a fundamental identifiability limit, not an architectural one.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
From Landscape to Dedicated Exploration
</div>
</div>
<div class="callout-body-container callout-body">
<p>This case study builds upon the <strong>348-iteration landscape exploration</strong> (<a href="./results.html">Landscape Results</a>), which systematically mapped GNN training configurations across 29 regimes. Blocks 7, 8, and 17 of that exploration devoted 36 iterations to the sparse regime, exhausting the configuration parameter space and establishing the conn_R<sup>2</sup>=0.489 ceiling that motivated the pivot to direct code modifications documented here.</p>
</div>
</div>
<hr>
</section>
<section id="why-code-changes" class="level2">
<h2 class="anchored" data-anchor-id="why-code-changes">Why Code Changes?</h2>
<p>The landscape exploration exhausted the configuration parameter space across 36 iterations in 3 blocks:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 20%">
<col style="width: 21%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Block</th>
<th style="text-align: center;">n_frames</th>
<th style="text-align: center;">eff_rank</th>
<th style="text-align: center;">Best conn</th>
<th style="text-align: center;">Degeneracy</th>
<th style="text-align: left;">Key</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.466</td>
<td style="text-align: center;">12/12</td>
<td style="text-align: left;">All configs give conn~0.489</td>
</tr>
<tr class="even">
<td style="text-align: center;">8 (noise)</td>
<td style="text-align: center;">10k</td>
<td style="text-align: center;">91</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0/12</td>
<td style="text-align: left;">Noise inflates eff_rank; no rescue</td>
</tr>
<tr class="odd">
<td style="text-align: center;">17</td>
<td style="text-align: center;"><strong>30k</strong></td>
<td style="text-align: center;"><strong>13</strong></td>
<td style="text-align: center;"><strong>0.436</strong></td>
<td style="text-align: center;"><strong>12/12</strong></td>
<td style="text-align: left;"><strong>eff_rank DROPS; 30k useless</strong></td>
</tr>
</tbody>
</table>
<p>The LLM identified the root cause as an <strong>architectural degeneracy</strong> in the GNN’s dual-pathway design:</p>
<p><span class="math display">\[\frac{du}{dt} = \text{lin\_phi}(u, a) + W \odot \text{lin\_edge}(u, a)\]</span></p>
<p>The <code>lin_phi</code> pathway can predict dynamics perfectly (test_pearson ≈ 1.000) without requiring correct W. At subcritical spectral radius, the W-mediated signal is too weak to overcome this compensation. The degeneracy gap is ~0.51.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
From Config to Code
</div>
</div>
<div class="callout-body-container callout-body">
<p>The dedicated exploration deliberately shifted from hyperparameter tuning to <strong>direct GNN code modification</strong>: changing initialization, optimization, regularization mechanics, and MLP architecture in the source files (<code>Signal_Propagation.py</code>, <code>graph_trainer.py</code>, <code>MLP.py</code>).</p>
</div>
</div>
<hr>
</section>
<section id="config-baseline-iterations-18" class="level2">
<h2 class="anchored" data-anchor-id="config-baseline-iterations-18">Config Baseline (Iterations 1–8)</h2>
<p>Before modifying code, the exploration confirmed the ceiling across 5 orders of L1 magnitude, 2–12 epochs, edge_diff up to 50000, and aggressive multi-parameter combinations. <strong>All 8 iterations produced conn_R<sup>2</sup> = 0.489.</strong> This confirmed the ceiling is not a training configuration problem, justifying the pivot to code changes.</p>
<hr>
</section>
<section id="code-modification-1-w-initialization-scale" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-1-w-initialization-scale">Code Modification 1: W Initialization Scale</h2>
<p><strong>Iterations 9–12</strong> · File: <code>Signal_Propagation.py</code></p>
<p><strong>Hypothesis</strong>: The true W matrix has entries ~ N(0, g/sqrt(n)) ≈ N(0, 0.7), but initialization uses N(0, 1.0) — a scale mismatch that forces early training to waste capacity correcting the scale before learning structure.</p>
<p><strong>Code change</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Before: W_init = torch.randn(n_neurons, n_neurons)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># After:</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>W_init <span class="op">=</span> torch.randn(n_neurons, n_neurons) <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">/</span> math.sqrt(<span class="va">self</span>.n_neurons))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Results</strong>: conn_R<sup>2</sup> = 0.489–0.490 across all 4 config variations (lr_W 1E-3 to 3E-3, L1 1E-4 to 1E-3, n_epochs 2–12).</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>Init scale is NOT the bottleneck. The optimizer corrects scale quickly; the problem is structural.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-2-proximal-l1-soft-thresholding" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-2-proximal-l1-soft-thresholding">Code Modification 2: Proximal L1 Soft-Thresholding</h2>
<p><strong>Iterations 13–16</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: Standard gradient-based L1 regularization penalizes W entries but never creates exact zeros. Since the true sparse W has 50% zeros, <strong>proximal soft-thresholding</strong> after each optimizer step should force exact sparsity, potentially aligning the learned structure with the true connectivity pattern.</p>
<p><strong>Code change</strong>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># After optimizer.step():</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    threshold <span class="op">=</span> coeff_W_L1 <span class="op">*</span> lr_W</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> model.W</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    W.data <span class="op">=</span> torch.sign(W.data) <span class="op">*</span> torch.clamp(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        torch.<span class="bu">abs</span>(W.data) <span class="op">-</span> threshold, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Results</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 10%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Iter</th>
<th style="text-align: center;">L1</th>
<th style="text-align: center;">Threshold</th>
<th style="text-align: center;">conn_R<sup>2</sup></th>
<th style="text-align: center;">Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">13</td>
<td style="text-align: center;">1E-4</td>
<td style="text-align: center;">3E-7</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">none — threshold negligible vs W scale (~0.1)</td>
</tr>
<tr class="even">
<td style="text-align: center;">14</td>
<td style="text-align: center;">1E-3</td>
<td style="text-align: center;">3E-6</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">none — still too small</td>
</tr>
<tr class="odd">
<td style="text-align: center;">15</td>
<td style="text-align: center;"><strong>1E-2</strong></td>
<td style="text-align: center;"><strong>3E-5</strong></td>
<td style="text-align: center;"><strong>0.361</strong></td>
<td style="text-align: center;"><strong>destructive</strong> — training collapses</td>
</tr>
<tr class="even">
<td style="text-align: center;">16</td>
<td style="text-align: center;">1E-3</td>
<td style="text-align: center;">3E-6 (no warm-up)</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">none — warm-up phase irrelevant</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect at moderate; destructive at extreme
</div>
</div>
<div class="callout-body-container callout-body">
<p>The fundamental problem: proximal threshold = L1 × lr_W produces values 3–4 orders below the W entry scale (~0.1). To reach effective thresholds, L1 must be so large that it destroys the loss landscape. <strong>The sparsity problem cannot be solved by regularization mechanics alone.</strong></p>
</div>
</div>
<hr>
</section>
<section id="code-modification-3-mlp-capacity-reduction" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-3-mlp-capacity-reduction">Code Modification 3: MLP Capacity Reduction</h2>
<p><strong>Iterations 17–20</strong> · Files: <code>Signal_Propagation.py</code>, <code>MLP.py</code></p>
<p><strong>Hypothesis</strong>: If <code>lin_phi</code> (the bypass pathway) has too much capacity, it absorbs all predictive power and leaves W unconstrained. <strong>Reducing MLP width or depth</strong> should force the model to route signal through W, breaking the degeneracy.</p>
<p><strong>Code changes</strong>: Reduced <code>hidden_dim</code> (64→32→16) and <code>n_layers</code> (3→2) in the MLP configuration.</p>
<p><strong>Results</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 23%">
<col style="width: 29%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Iter</th>
<th>Change</th>
<th style="text-align: center;">conn_R<sup>2</sup></th>
<th style="text-align: center;">test_pearson</th>
<th style="text-align: center;">Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">17</td>
<td>hidden_dim 64→32</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">none — MLP still compensates</td>
</tr>
<tr class="even">
<td style="text-align: center;">18</td>
<td>hidden_dim 32 + L1=1E-3</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">none — combined still fails</td>
</tr>
<tr class="odd">
<td style="text-align: center;">19</td>
<td><strong>n_layers 3→2</strong></td>
<td style="text-align: center;"><strong>0.010</strong></td>
<td style="text-align: center;"><strong>-0.035</strong></td>
<td style="text-align: center;"><strong>catastrophic failure</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">20</td>
<td><strong>hidden_dim 64→16</strong></td>
<td style="text-align: center;"><strong>0.017</strong></td>
<td style="text-align: center;"><strong>-0.072</strong></td>
<td style="text-align: center;"><strong>catastrophic failure</strong></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Established Principle: MLP has a hard minimum capacity
</div>
</div>
<div class="callout-body-container callout-body">
<p>3 layers and hidden_dim≥32 are structurally necessary for the GNN to function at all. Below these thresholds, the model cannot even predict dynamics. Above the minimum, the MLP <strong>always</strong> fully compensates for incorrect W. There is no “Goldilocks zone” where MLP capacity is enough for dynamics but insufficient for W bypass.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-4-gradient-clipping-on-w" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-4-gradient-clipping-on-w">Code Modification 4: Gradient Clipping on W</h2>
<p><strong>Iterations 21–24</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: W has n<sup>2</sup>=10,000 entries receiving gradients from all neuron pairs. Large, noisy gradients may destabilize W learning, preventing convergence to the true sparse pattern. <strong>Gradient clipping</strong> should stabilize the W update trajectory.</p>
<p><strong>Code change</strong>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Before optimizer.step():</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_([model.W], max_norm<span class="op">=</span><span class="fl">1.0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Results</strong>: conn_R<sup>2</sup> = 0.488–0.489 across all 4 config variations (baseline, L1=1E-3, lr_W=5E-3, n_epochs=12).</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>Gradient noise is not the bottleneck. The W gradients carry correct directional information — the problem is that the loss landscape does not require W to be correct.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-5-lin_phi-scaling-removal" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-5-lin_phi-scaling-removal">Code Modification 5: lin_phi Scaling / Removal</h2>
<p><strong>Iterations 25–28</strong> · File: <code>Signal_Propagation.py</code>, <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: The dual-pathway architecture <code>du/dt = lin_phi(u,a) + W @ lin_edge(u,a)</code> allows <code>lin_phi</code> to bypass W entirely. By multiplying <code>lin_phi</code> output by a scaling factor (0.0–0.5), or removing it completely, the model should be forced to route signal through the <code>W @ lin_edge</code> pathway, making correct W recovery essential.</p>
<p><strong>Code change</strong>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Signal_Propagation.py: multiply lin_phi output by phi_scale</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>phi_output <span class="op">=</span> <span class="va">self</span>.phi_scale <span class="op">*</span> <span class="va">self</span>.lin_phi(x, a)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>du_dt <span class="op">=</span> phi_output <span class="op">+</span> W <span class="op">@</span> <span class="va">self</span>.lin_edge(x, a)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Results</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 18%">
<col style="width: 6%">
<col style="width: 18%">
<col style="width: 23%">
<col style="width: 8%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Iter</th>
<th style="text-align: center;">phi_scale</th>
<th style="text-align: center;">L1</th>
<th style="text-align: center;">conn_R<sup>2</sup></th>
<th style="text-align: center;">test_pearson</th>
<th style="text-align: center;">Gap</th>
<th style="text-align: center;">Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">25</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1E-4</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.9997</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">none</td>
</tr>
<tr class="even">
<td style="text-align: center;">26</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1E-4</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">0.9996</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">none</td>
</tr>
<tr class="odd">
<td style="text-align: center;">27</td>
<td style="text-align: center;"><strong>0.0</strong></td>
<td style="text-align: center;">1E-4</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.9994</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;"><strong>none — complete removal</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">28</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1E-3</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.9986</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">none — L1 still irrelevant</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Established Principle: lin_phi is NOT the degeneracy source
</div>
</div>
<div class="callout-body-container callout-body">
<p>Even <strong>complete removal</strong> of lin_phi (phi_scale=0.0, so <code>du/dt = W @ lin_edge(u,a)</code> only) produces conn_R<sup>2</sup>=0.489. The degeneracy is entirely within <strong>lin_edge</strong>: the flexible MLP can absorb any W, so <code>W_wrong @ f(u,a) ≈ W_true @ g(u,a)</code> regardless of whether lin_phi exists. This falsifies the initial architectural hypothesis and redirects the diagnosis from the bypass pathway to the message-passing MLP itself.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-6-recurrent-training" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-6-recurrent-training">Code Modification 6: Recurrent Training</h2>
<p><strong>Iterations 29–32</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: Multi-step rollout (time_step=4, 16, 32) should compound errors from wrong W over multiple prediction steps. Single-step training allows <code>W_wrong @ f(u,a) ≈ W_true @ g(u,a)</code> at each step independently; recurrent training penalizes wrong W because errors accumulate.</p>
<p><strong>Results</strong>: conn_R<sup>2</sup> = 0.489 at time_step=4 and 16; slight degradation at time_step=32. noise_recurrent_level&gt;0.01 is destructive.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect or destructive
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recurrent training does not help in the subcritical regime. Error compounding requires the W-mediated signal to be strong enough that wrong W produces detectable trajectory divergence — at rho=0.746, the signal is too weak.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-7-anti-sparsity-penalty-on-w" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-7-anti-sparsity-penalty-on-w">Code Modification 7: Anti-Sparsity Penalty on W</h2>
<p><strong>Iterations 33–36</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: If the optimizer is trapped in a degenerate W solution, an <strong>anti-sparsity penalty</strong> during phase 1 (penalizing W entries near zero) could force W to explore denser regions of parameter space before L1 refines sparsity in phase 2, disrupting the (W, lin_edge) equilibrium.</p>
<p><strong>Results</strong>: conn_R<sup>2</sup> drops to 0.023–0.181 across all configurations.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Universally destructive
</div>
</div>
<div class="callout-body-container callout-body">
<p>Anti-sparsity penalty destroys W recovery entirely. Forcing W away from zero during warm-up does not disrupt the equilibrium — it creates a worse starting point that L1 cannot recover from.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-8-freeze-lin_edge-after-warm-up" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-8-freeze-lin_edge-after-warm-up">Code Modification 8: Freeze lin_edge After Warm-Up</h2>
<p><strong>Iterations 37–40</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: If lin_edge adapts during training to compensate for wrong W, <strong>freezing lin_edge after the warm-up phase</strong> should force subsequent training to correct W rather than adjusting the MLP.</p>
<p><strong>Results</strong>: conn_R<sup>2</sup> = 0.489 across all 4 configurations.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>The degenerate lin_edge function is already learned during the warm-up phase. Freezing it afterward does not change W — the compensation is baked in before the freeze takes effect.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-9-lin_edge-dropout" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-9-lin_edge-dropout">Code Modification 9: lin_edge Dropout</h2>
<p><strong>Iterations 41–44</strong> · File: <code>graph_trainer.py</code></p>
<p><strong>Hypothesis</strong>: Applying dropout (p=0.3, 0.5) to lin_edge during training should randomly disable MLP units, preventing the MLP from developing a coherent compensation strategy and forcing W to carry more signal.</p>
<p><strong>Results</strong>: conn_R<sup>2</sup> = 0.489 at both p=0.3 and p=0.5.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Verdict: Zero effect
</div>
</div>
<div class="callout-body-container callout-body">
<p>The MLP compensation is distributed and redundant — dropout cannot disrupt it. The lin_edge function learns a robust compensation that survives random unit masking.</p>
</div>
</div>
<hr>
</section>
<section id="code-modification-10-lin_edge-mode-bypass-paradigm-shift" class="level2">
<h2 class="anchored" data-anchor-id="code-modification-10-lin_edge-mode-bypass-paradigm-shift">Code Modification 10: lin_edge Mode Bypass (Paradigm Shift)</h2>
<p><strong>Iterations 45–48</strong> · Files: <code>Signal_Propagation.py</code>, <code>config.py</code></p>
<p><strong>Hypothesis</strong>: If lin_edge MLP compensation is the degeneracy source, replacing it with a <strong>fixed nonlinearity</strong> (tanh or identity) should eliminate compensation entirely and force W to be correct.</p>
<p><strong>Code change</strong>: Added <code>lin_edge_mode</code> parameter to select between MLP (default), fixed tanh, or identity for the edge message function.</p>
<p><strong>Results</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 24%">
<col style="width: 18%">
<col style="width: 24%">
<col style="width: 8%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Iter</th>
<th style="text-align: center;">lin_edge_mode</th>
<th style="text-align: center;">conn_R<sup>2</sup></th>
<th style="text-align: center;">test_pearson</th>
<th style="text-align: center;">Gap</th>
<th style="text-align: center;">Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">45</td>
<td style="text-align: center;">tanh</td>
<td style="text-align: center;"><strong>0.489</strong></td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;"><strong>conn IDENTICAL; dynamics much worse</strong></td>
</tr>
<tr class="even">
<td style="text-align: center;">46</td>
<td style="text-align: center;">tanh + L1=1E-3</td>
<td style="text-align: center;"><strong>0.489</strong></td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">L1 still irrelevant at tanh</td>
</tr>
<tr class="odd">
<td style="text-align: center;">47</td>
<td style="text-align: center;">tanh + 12ep</td>
<td style="text-align: center;"><strong>0.489</strong></td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.065</td>
<td style="text-align: center;">more training does not help</td>
</tr>
<tr class="even">
<td style="text-align: center;">48</td>
<td style="text-align: center;"><strong>identity</strong></td>
<td style="text-align: center;"><strong>0.009</strong></td>
<td style="text-align: center;">-0.035</td>
<td style="text-align: center;">–</td>
<td style="text-align: center;"><strong>catastrophic failure</strong></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Paradigm Shift: The Degeneracy Gap Was a Red Herring
</div>
</div>
<div class="callout-body-container callout-body">
<p>Replacing the MLP with a <strong>fixed tanh nonlinearity</strong> gives <strong>identical conn_R<sup>2</sup>=0.489</strong> while test_pearson drops from 1.000 to 0.423. This fundamentally changes the diagnosis:</p>
<ul>
<li>The MLP was <strong>not causing</strong> the W recovery ceiling — it was only <strong>improving dynamics prediction</strong> on top of it</li>
<li>The degeneracy gap (test_pearson - conn_R<sup>2</sup> ≈ 0.51) was a red herring: it reflected MLP compensation <em>improving dynamics</em>, not <em>preventing W recovery</em></li>
<li>With fixed tanh, the gap collapses to ~0.065 (both metrics are low), confirming that the ceiling is about <strong>identifiability</strong>, not compensation</li>
</ul>
<p>The 0.489 ceiling is a <strong>fundamental identifiability limit</strong> of the model form <code>du/dt = W @ f(u,a)</code> at this data regime, not an architectural degeneracy.</p>
</div>
</div>
<hr>
</section>
<section id="seed-universality-confirmation" class="level2">
<h2 class="anchored" data-anchor-id="seed-universality-confirmation">Seed Universality Confirmation</h2>
<p><strong>Iterations 49–52</strong> · Seed variation (42, 137, 256)</p>
<p>To confirm the ceiling is not specific to one random W realization, three different seeds were tested:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Iter</th>
<th style="text-align: center;">Seed</th>
<th style="text-align: center;">n_epochs</th>
<th style="text-align: center;">conn_R<sup>2</sup></th>
<th style="text-align: center;">test_pearson</th>
<th style="text-align: center;">Effect</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">49</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">identical ceiling</td>
</tr>
<tr class="even">
<td style="text-align: center;">50</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">12 epochs = 6 epochs</td>
</tr>
<tr class="odd">
<td style="text-align: center;">51</td>
<td style="text-align: center;">137</td>
<td style="text-align: center;">6 (lr_W=1E-2)</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">3x lr_W = zero effect</td>
</tr>
<tr class="even">
<td style="text-align: center;">52</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">identical ceiling</td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Established: The 0.489 Ceiling Is Universal
</div>
</div>
<div class="callout-body-container callout-body">
<p>Three different random W realizations all produce conn_R<sup>2</sup>=0.489. The ceiling is a property of the model form + data regime, not of any specific connectivity matrix.</p>
</div>
</div>
<hr>
</section>
<section id="summary-what-failed-and-why" class="level2">
<h2 class="anchored" data-anchor-id="summary-what-failed-and-why">Summary: What Failed and Why</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 29%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Code Modification</th>
<th style="text-align: center;">Iters</th>
<th>Hypothesis</th>
<th>Outcome</th>
<th>Reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>W init scale (1/sqrt(n))</td>
<td style="text-align: center;">9–12</td>
<td>Scale mismatch wastes early training</td>
<td>Zero effect</td>
<td>Optimizer corrects scale quickly</td>
</tr>
<tr class="even">
<td>Proximal L1 soft-thresholding</td>
<td style="text-align: center;">13–16</td>
<td>Exact zeros needed for sparsity</td>
<td>Zero/destructive</td>
<td>Threshold too small vs W scale</td>
</tr>
<tr class="odd">
<td>MLP hidden_dim reduction</td>
<td style="text-align: center;">17–20</td>
<td>Force signal through W pathway</td>
<td>Zero/catastrophic</td>
<td>No Goldilocks zone exists</td>
</tr>
<tr class="even">
<td>Gradient clipping on W</td>
<td style="text-align: center;">21–24</td>
<td>Stabilize W trajectory</td>
<td>Zero effect</td>
<td>Gradients are directionally correct</td>
</tr>
<tr class="odd">
<td>lin_phi scaling/removal</td>
<td style="text-align: center;">25–28</td>
<td>Bypass pathway absorbs signal</td>
<td>Zero effect</td>
<td>Degeneracy is in lin_edge, not lin_phi</td>
</tr>
<tr class="even">
<td>Recurrent training</td>
<td style="text-align: center;">29–32</td>
<td>Compound errors from wrong W</td>
<td>Zero/destructive</td>
<td>W signal too weak for error compounding</td>
</tr>
<tr class="odd">
<td>Anti-sparsity penalty</td>
<td style="text-align: center;">33–36</td>
<td>Disrupt (W, lin_edge) equilibrium</td>
<td>Destructive</td>
<td>Creates worse starting point</td>
</tr>
<tr class="even">
<td>Freeze lin_edge</td>
<td style="text-align: center;">37–40</td>
<td>Force W correction after warm-up</td>
<td>Zero effect</td>
<td>Degenerate function already baked in</td>
</tr>
<tr class="odd">
<td>lin_edge dropout</td>
<td style="text-align: center;">41–44</td>
<td>Prevent coherent compensation</td>
<td>Zero effect</td>
<td>Compensation is distributed/redundant</td>
</tr>
<tr class="even">
<td><strong>lin_edge mode bypass</strong></td>
<td style="text-align: center;"><strong>45–48</strong></td>
<td><strong>Eliminate MLP compensation</strong></td>
<td><strong>PARADIGM SHIFT</strong></td>
<td><strong>Ceiling is identifiability, not compensation</strong></td>
</tr>
</tbody>
</table>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Result
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>All 52 iterations produced conn_R<sup>2</sup> = 0.489</strong> (±0.001, excluding catastrophic failures from aggressive MLP reduction, anti-sparsity penalty, and identity lin_edge). Ten distinct code modifications — targeting initialization, regularization mechanics, model capacity, optimization stability, the lin_phi bypass pathway, recurrent training, anti-sparsity, lin_edge freezing, lin_edge dropout, and lin_edge replacement — all failed. The final paradigm shift (lin_edge_mode=tanh) revealed that the ceiling is a <strong>fundamental identifiability limit</strong>, not an architectural degeneracy. The degeneracy gap was a red herring.</p>
</div>
</div>
<hr>
</section>
<section id="revised-architectural-diagnosis" class="level2">
<h2 class="anchored" data-anchor-id="revised-architectural-diagnosis">Revised Architectural Diagnosis</h2>
<p>The exploration converges on a <strong>revised</strong> diagnosis after the lin_edge_mode paradigm shift. The ceiling is <strong>not</strong> caused by MLP compensation — it is a fundamental identifiability limit:</p>
<ol type="1">
<li><strong>W signal is weak</strong>: subcritical spectral radius (rho=0.746) means W-mediated dynamics carry limited information</li>
<li><strong>The identifiability limit is model-form-dependent</strong>: <code>du/dt = W @ f(u,a)</code> with a fixed f(u,a) = tanh(u) gives identical conn_R<sup>2</sup>=0.489 — the product <code>W @ tanh(u)</code> cannot recover the true W from 10,000 frames of 100-neuron sparse activity regardless of the function form</li>
<li><strong>MLP compensation improves dynamics, not degrades W</strong>: the MLP lin_edge lifts test_pearson from 0.42 to 1.00 without changing conn_R<sup>2</sup>; it makes <em>predictions better</em> while W remains equally wrong</li>
<li><strong>The ceiling is universal across seeds</strong>: three different W realizations (seeds 42, 137, 256) all produce conn_R<sup>2</sup>=0.489</li>
</ol>
<p>This is qualitatively different from the dense regime (rho &gt; 1.0) where the W-mediated signal is strong enough to uniquely determine W from the dynamics.</p>
<hr>
</section>
<section id="ucb-exploration-trees" class="level2">
<h2 class="anchored" data-anchor-id="ucb-exploration-trees">UCB Exploration Trees</h2>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Block 1 (iter 12)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Block 2 (iter 24)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false">Block 3 (iter 36)</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" role="tab" aria-controls="tabset-1-4" aria-selected="false">Block 4 (iter 48)</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p><a href="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_012.png" class="lightbox" data-gallery="ucb-sp"><img src="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_012.png" class="img-fluid"></a></p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p><a href="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_024.png" class="lightbox" data-gallery="ucb-sp"><img src="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_024.png" class="img-fluid"></a></p>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<p><a href="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_036.png" class="lightbox" data-gallery="ucb-sp"><img src="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_036.png" class="img-fluid"></a></p>
</div>
<div id="tabset-1-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-4-tab">
<p><a href="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_048.png" class="lightbox" data-gallery="ucb-sp"><img src="log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_048.png" class="img-fluid"></a></p>
</div>
</div>
</div>
<hr>
</section>
<section id="next-steps-sgd-optimizer-for-w" class="level2">
<h2 class="anchored" data-anchor-id="next-steps-sgd-optimizer-for-w">Next Steps: SGD Optimizer for W</h2>
<p>The remaining unexplored dimension is the optimizer itself. A <strong>separate SGD optimizer</strong> (with momentum) for W may produce sharper gradients than Adam’s adaptive learning rate, potentially enabling L1 to produce more exact zeros. This is the next planned code modification (block 5, iterations 53–56). However, given that the ceiling is now understood to be an identifiability limit rather than an optimization failure, expectations are low.</p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Case Study: Sparse Connectivity"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Breaking the 0.489 ceiling --- LLM-guided code modifications to the GNN architecture (52 iterations)"</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="fu">## Problem Statement</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>Sparse connectivity (filling_factor=50%, n=100 neurons) produces neural activity with subcritical spectral radius (rho=0.746). The landscape exploration (blocks 7, 8, 17) established that **no configuration sweep can break the conn_R^2^=0.489 ceiling** --- neither regularization, learning rates, epochs, noise, nor tripling the data (30k frames) had any effect. This motivated a dedicated exploration focused on **modifying the GNN code itself**.</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>The central question: **can code-level modifications to the GNN architecture break the 0.489 ceiling?** After 52 iterations testing 10 code modifications across 3 random seeds, the answer is **no** --- the ceiling is a fundamental identifiability limit, not an architectural one.</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Landscape to Dedicated Exploration</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>This case study builds upon the **348-iteration landscape exploration** (<span class="co">[</span><span class="ot">Landscape Results</span><span class="co">](results.qmd)</span>), which systematically mapped GNN training configurations across 29 regimes. Blocks 7, 8, and 17 of that exploration devoted 36 iterations to the sparse regime, exhausting the configuration parameter space and establishing the conn_R^2^=0.489 ceiling that motivated the pivot to direct code modifications documented here.</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Code Changes?</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>The landscape exploration exhausted the configuration parameter space across 36 iterations in 3 blocks:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>| Block | n_frames | eff_rank | Best conn | Degeneracy | Key |</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>|:-----:|:--------:|:--------:|:---------:|:----------:|:----|</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>| 7 | 10k | 21 | 0.466 | 12/12 | All configs give conn~0.489 |</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>| 8 (noise) | 10k | 91 | 0.490 | 0/12 | Noise inflates eff_rank; no rescue |</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>| 17 | **30k** | **13** | **0.436** | **12/12** | **eff_rank DROPS; 30k useless** |</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>The LLM identified the root cause as an **architectural degeneracy** in the GNN's dual-pathway design:</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>$$\frac{du}{dt} = \text{lin<span class="sc">\_</span>phi}(u, a) + W \odot \text{lin<span class="sc">\_</span>edge}(u, a)$$</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>The <span class="in">`lin_phi`</span> pathway can predict dynamics perfectly (test_pearson ≈ 1.000) without requiring correct W. At subcritical spectral radius, the W-mediated signal is too weak to overcome this compensation. The degeneracy gap is ~0.51.</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Config to Code</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>The dedicated exploration deliberately shifted from hyperparameter tuning to **direct GNN code modification**: changing initialization, optimization, regularization mechanics, and MLP architecture in the source files (<span class="in">`Signal_Propagation.py`</span>, <span class="in">`graph_trainer.py`</span>, <span class="in">`MLP.py`</span>).</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## Config Baseline (Iterations 1--8)</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>Before modifying code, the exploration confirmed the ceiling across 5 orders of L1 magnitude, 2--12 epochs, edge_diff up to 50000, and aggressive multi-parameter combinations. **All 8 iterations produced conn_R^2^ = 0.489.** This confirmed the ceiling is not a training configuration problem, justifying the pivot to code changes.</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 1: W Initialization Scale</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>**Iterations 9--12** · File: <span class="in">`Signal_Propagation.py`</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: The true W matrix has entries ~ N(0, g/sqrt(n)) ≈ N(0, 0.7), but initialization uses N(0, 1.0) --- a scale mismatch that forces early training to waste capacity correcting the scale before learning structure.</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>**Code change**:</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Before: W_init = torch.randn(n_neurons, n_neurons)</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a><span class="co"># After:</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>W_init <span class="op">=</span> torch.randn(n_neurons, n_neurons) <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">/</span> math.sqrt(<span class="va">self</span>.n_neurons))</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ = 0.489--0.490 across all 4 config variations (lr_W 1E-3 to 3E-3, L1 1E-4 to 1E-3, n_epochs 2--12).</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>Init scale is NOT the bottleneck. The optimizer corrects scale quickly; the problem is structural.</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 2: Proximal L1 Soft-Thresholding</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>**Iterations 13--16** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: Standard gradient-based L1 regularization penalizes W entries but never creates exact zeros. Since the true sparse W has 50% zeros, **proximal soft-thresholding** after each optimizer step should force exact sparsity, potentially aligning the learned structure with the true connectivity pattern.</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>**Code change**:</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="co"># After optimizer.step():</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    threshold <span class="op">=</span> coeff_W_L1 <span class="op">*</span> lr_W</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> model.W</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>    W.data <span class="op">=</span> torch.sign(W.data) <span class="op">*</span> torch.clamp(</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>        torch.<span class="bu">abs</span>(W.data) <span class="op">-</span> threshold, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>**Results**:</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>| Iter | L1 | Threshold | conn_R^2^ | Effect |</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>|:----:|:--:|:---------:|:---------:|:------:|</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>| 13 | 1E-4 | 3E-7 | 0.490 | none --- threshold negligible vs W scale (~0.1) |</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>| 14 | 1E-3 | 3E-6 | 0.488 | none --- still too small |</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>| 15 | **1E-2** | **3E-5** | **0.361** | **destructive** --- training collapses |</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>| 16 | 1E-3 | 3E-6 (no warm-up) | 0.488 | none --- warm-up phase irrelevant |</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect at moderate; destructive at extreme</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>The fundamental problem: proximal threshold = L1 × lr_W produces values 3--4 orders below the W entry scale (~0.1). To reach effective thresholds, L1 must be so large that it destroys the loss landscape. **The sparsity problem cannot be solved by regularization mechanics alone.**</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 3: MLP Capacity Reduction</span></span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>**Iterations 17--20** · Files: <span class="in">`Signal_Propagation.py`</span>, <span class="in">`MLP.py`</span></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: If `lin_phi` (the bypass pathway) has too much capacity, it absorbs all predictive power and leaves W unconstrained. **Reducing MLP width or depth** should force the model to route signal through W, breaking the degeneracy.</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>**Code changes**: Reduced <span class="in">`hidden_dim`</span> (64→32→16) and <span class="in">`n_layers`</span> (3→2) in the MLP configuration.</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>**Results**:</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>| Iter | Change | conn_R^2^ | test_pearson | Effect |</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a>|:----:|--------|:---------:|:------------:|:------:|</span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>| 17 | hidden_dim 64→32 | 0.489 | 0.999 | none --- MLP still compensates |</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>| 18 | hidden_dim 32 + L1=1E-3 | 0.488 | 0.999 | none --- combined still fails |</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>| 19 | **n_layers 3→2** | **0.010** | **-0.035** | **catastrophic failure** |</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>| 20 | **hidden_dim 64→16** | **0.017** | **-0.072** | **catastrophic failure** |</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a><span class="fu">### Established Principle: MLP has a hard minimum capacity</span></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>3 layers and hidden_dim≥32 are structurally necessary for the GNN to function at all. Below these thresholds, the model cannot even predict dynamics. Above the minimum, the MLP **always** fully compensates for incorrect W. There is no "Goldilocks zone" where MLP capacity is enough for dynamics but insufficient for W bypass.</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 4: Gradient Clipping on W</span></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>**Iterations 21--24** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: W has n^2^=10,000 entries receiving gradients from all neuron pairs. Large, noisy gradients may destabilize W learning, preventing convergence to the true sparse pattern. **Gradient clipping** should stabilize the W update trajectory.</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>**Code change**:</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a><span class="co"># Before optimizer.step():</span></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>torch.nn.utils.clip_grad_norm_([model.W], max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ = 0.488--0.489 across all 4 config variations (baseline, L1=1E-3, lr_W=5E-3, n_epochs=12).</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect</span></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>Gradient noise is not the bottleneck. The W gradients carry correct directional information --- the problem is that the loss landscape does not require W to be correct.</span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 5: lin_phi Scaling / Removal</span></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a>**Iterations 25--28** · File: <span class="in">`Signal_Propagation.py`</span>, <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: The dual-pathway architecture <span class="in">`du/dt = lin_phi(u,a) + W @ lin_edge(u,a)`</span> allows <span class="in">`lin_phi`</span> to bypass W entirely. By multiplying <span class="in">`lin_phi`</span> output by a scaling factor (0.0--0.5), or removing it completely, the model should be forced to route signal through the <span class="in">`W @ lin_edge`</span> pathway, making correct W recovery essential.</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>**Code change**:</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a><span class="co"># Signal_Propagation.py: multiply lin_phi output by phi_scale</span></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a>phi_output <span class="op">=</span> <span class="va">self</span>.phi_scale <span class="op">*</span> <span class="va">self</span>.lin_phi(x, a)</span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>du_dt <span class="op">=</span> phi_output <span class="op">+</span> W <span class="op">@</span> <span class="va">self</span>.lin_edge(x, a)</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a>**Results**:</span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a>| Iter | phi_scale | L1 | conn_R^2^ | test_pearson | Gap | Effect |</span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>|:----:|:---------:|:--:|:---------:|:------------:|:---:|:------:|</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a>| 25 | 0.1 | 1E-4 | 0.489 | 0.9997 | 0.511 | none |</span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>| 26 | 0.5 | 1E-4 | 0.490 | 0.9996 | 0.511 | none |</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>| 27 | **0.0** | 1E-4 | 0.489 | 0.9994 | 0.510 | **none --- complete removal** |</span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>| 28 | 0.1 | 1E-3 | 0.488 | 0.9986 | 0.511 | none --- L1 still irrelevant |</span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### Established Principle: lin_phi is NOT the degeneracy source</span></span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a>Even **complete removal** of lin_phi (phi_scale=0.0, so `du/dt = W @ lin_edge(u,a)` only) produces conn_R^2^=0.489. The degeneracy is entirely within **lin_edge**: the flexible MLP can absorb any W, so <span class="in">`W_wrong @ f(u,a) ≈ W_true @ g(u,a)`</span> regardless of whether lin_phi exists. This falsifies the initial architectural hypothesis and redirects the diagnosis from the bypass pathway to the message-passing MLP itself.</span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 6: Recurrent Training</span></span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>**Iterations 29--32** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: Multi-step rollout (time_step=4, 16, 32) should compound errors from wrong W over multiple prediction steps. Single-step training allows <span class="in">`W_wrong @ f(u,a) ≈ W_true @ g(u,a)`</span> at each step independently; recurrent training penalizes wrong W because errors accumulate.</span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ = 0.489 at time_step=4 and 16; slight degradation at time_step=32. noise_recurrent_level&gt;0.01 is destructive.</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect or destructive</span></span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>Recurrent training does not help in the subcritical regime. Error compounding requires the W-mediated signal to be strong enough that wrong W produces detectable trajectory divergence --- at rho=0.746, the signal is too weak.</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 7: Anti-Sparsity Penalty on W</span></span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a>**Iterations 33--36** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: If the optimizer is trapped in a degenerate W solution, an **anti-sparsity penalty** during phase 1 (penalizing W entries near zero) could force W to explore denser regions of parameter space before L1 refines sparsity in phase 2, disrupting the (W, lin_edge) equilibrium.</span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ drops to 0.023--0.181 across all configurations.</span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Universally destructive</span></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a>Anti-sparsity penalty destroys W recovery entirely. Forcing W away from zero during warm-up does not disrupt the equilibrium --- it creates a worse starting point that L1 cannot recover from.</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 8: Freeze lin_edge After Warm-Up</span></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>**Iterations 37--40** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: If lin_edge adapts during training to compensate for wrong W, **freezing lin_edge after the warm-up phase** should force subsequent training to correct W rather than adjusting the MLP.</span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ = 0.489 across all 4 configurations.</span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect</span></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a>The degenerate lin_edge function is already learned during the warm-up phase. Freezing it afterward does not change W --- the compensation is baked in before the freeze takes effect.</span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 9: lin_edge Dropout</span></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a>**Iterations 41--44** · File: <span class="in">`graph_trainer.py`</span></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: Applying dropout (p=0.3, 0.5) to lin_edge during training should randomly disable MLP units, preventing the MLP from developing a coherent compensation strategy and forcing W to carry more signal.</span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a>**Results**: conn_R^2^ = 0.489 at both p=0.3 and p=0.5.</span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### Verdict: Zero effect</span></span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a>The MLP compensation is distributed and redundant --- dropout cannot disrupt it. The lin_edge function learns a robust compensation that survives random unit masking.</span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Modification 10: lin_edge Mode Bypass (Paradigm Shift)</span></span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a>**Iterations 45--48** · Files: <span class="in">`Signal_Propagation.py`</span>, <span class="in">`config.py`</span></span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>**Hypothesis**: If lin_edge MLP compensation is the degeneracy source, replacing it with a **fixed nonlinearity** (tanh or identity) should eliminate compensation entirely and force W to be correct.</span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>**Code change**: Added <span class="in">`lin_edge_mode`</span> parameter to select between MLP (default), fixed tanh, or identity for the edge message function.</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a>**Results**:</span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a>| Iter | lin_edge_mode | conn_R^2^ | test_pearson | Gap | Effect |</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a>|:----:|:------------:|:---------:|:------------:|:---:|:------:|</span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a>| 45 | tanh | **0.489** | 0.423 | 0.066 | **conn IDENTICAL; dynamics much worse** |</span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a>| 46 | tanh + L1=1E-3 | **0.489** | 0.424 | 0.065 | L1 still irrelevant at tanh |</span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>| 47 | tanh + 12ep | **0.489** | 0.424 | 0.065 | more training does not help |</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a>| 48 | **identity** | **0.009** | -0.035 | -- | **catastrophic failure** |</span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a><span class="fu">### Paradigm Shift: The Degeneracy Gap Was a Red Herring</span></span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a>Replacing the MLP with a **fixed tanh nonlinearity** gives **identical conn_R^2^=0.489** while test_pearson drops from 1.000 to 0.423. This fundamentally changes the diagnosis:</span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The MLP was **not causing** the W recovery ceiling --- it was only **improving dynamics prediction** on top of it</span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The degeneracy gap (test_pearson - conn_R^2^ ≈ 0.51) was a red herring: it reflected MLP compensation *improving dynamics*, not *preventing W recovery*</span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With fixed tanh, the gap collapses to ~0.065 (both metrics are low), confirming that the ceiling is about **identifiability**, not compensation</span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a>The 0.489 ceiling is a **fundamental identifiability limit** of the model form <span class="in">`du/dt = W @ f(u,a)`</span> at this data regime, not an architectural degeneracy.</span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a><span class="fu">## Seed Universality Confirmation</span></span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a>**Iterations 49--52** · Seed variation (42, 137, 256)</span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a>To confirm the ceiling is not specific to one random W realization, three different seeds were tested:</span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a>| Iter | Seed | n_epochs | conn_R^2^ | test_pearson | Effect |</span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a>|:----:|:----:|:--------:|:---------:|:------------:|:------:|</span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a>| 49 | 42 | 6 | 0.489 | 0.423 | identical ceiling |</span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a>| 50 | 137 | 12 | 0.489 | 0.424 | 12 epochs = 6 epochs |</span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a>| 51 | 137 | 6 (lr_W=1E-2) | 0.489 | 0.425 | 3x lr_W = zero effect |</span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a>| 52 | 256 | 6 | 0.489 | 0.429 | identical ceiling |</span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a><span class="fu">### Established: The 0.489 Ceiling Is Universal</span></span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a>Three different random W realizations all produce conn_R^2^=0.489. The ceiling is a property of the model form + data regime, not of any specific connectivity matrix.</span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary: What Failed and Why</span></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a>| Code Modification | Iters | Hypothesis | Outcome | Reason |</span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a>|---|:---:|---|---|---|</span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a>| W init scale (1/sqrt(n)) | 9--12 | Scale mismatch wastes early training | Zero effect | Optimizer corrects scale quickly |</span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a>| Proximal L1 soft-thresholding | 13--16 | Exact zeros needed for sparsity | Zero/destructive | Threshold too small vs W scale |</span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a>| MLP hidden_dim reduction | 17--20 | Force signal through W pathway | Zero/catastrophic | No Goldilocks zone exists |</span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a>| Gradient clipping on W | 21--24 | Stabilize W trajectory | Zero effect | Gradients are directionally correct |</span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a>| lin_phi scaling/removal | 25--28 | Bypass pathway absorbs signal | Zero effect | Degeneracy is in lin_edge, not lin_phi |</span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>| Recurrent training | 29--32 | Compound errors from wrong W | Zero/destructive | W signal too weak for error compounding |</span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a>| Anti-sparsity penalty | 33--36 | Disrupt (W, lin_edge) equilibrium | Destructive | Creates worse starting point |</span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a>| Freeze lin_edge | 37--40 | Force W correction after warm-up | Zero effect | Degenerate function already baked in |</span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a>| lin_edge dropout | 41--44 | Prevent coherent compensation | Zero effect | Compensation is distributed/redundant |</span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a>| **lin_edge mode bypass** | **45--48** | **Eliminate MLP compensation** | **PARADIGM SHIFT** | **Ceiling is identifiability, not compensation** |</span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Result</span></span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>**All 52 iterations produced conn_R^2^ = 0.489** (±0.001, excluding catastrophic failures from aggressive MLP reduction, anti-sparsity penalty, and identity lin_edge). Ten distinct code modifications --- targeting initialization, regularization mechanics, model capacity, optimization stability, the lin_phi bypass pathway, recurrent training, anti-sparsity, lin_edge freezing, lin_edge dropout, and lin_edge replacement --- all failed. The final paradigm shift (lin_edge_mode=tanh) revealed that the ceiling is a **fundamental identifiability limit**, not an architectural degeneracy. The degeneracy gap was a red herring.</span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a><span class="fu">## Revised Architectural Diagnosis</span></span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a>The exploration converges on a **revised** diagnosis after the lin_edge_mode paradigm shift. The ceiling is **not** caused by MLP compensation --- it is a fundamental identifiability limit:</span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**W signal is weak**: subcritical spectral radius (rho=0.746) means W-mediated dynamics carry limited information</span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**The identifiability limit is model-form-dependent**: <span class="in">`du/dt = W @ f(u,a)`</span> with a fixed f(u,a) = tanh(u) gives identical conn_R^2^=0.489 --- the product <span class="in">`W @ tanh(u)`</span> cannot recover the true W from 10,000 frames of 100-neuron sparse activity regardless of the function form</span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**MLP compensation improves dynamics, not degrades W**: the MLP lin_edge lifts test_pearson from 0.42 to 1.00 without changing conn_R^2^; it makes *predictions better* while W remains equally wrong</span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**The ceiling is universal across seeds**: three different W realizations (seeds 42, 137, 256) all produce conn_R^2^=0.489</span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a>This is qualitatively different from the dense regime (rho &gt; 1.0) where the W-mediated signal is strong enough to uniquely determine W from the dynamics.</span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## UCB Exploration Trees</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a>::: {.panel-tabset}</span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block 1 (iter 12)</span></span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a><span class="al">![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_012.png)</span>{.lightbox group="ucb-sp"}</span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block 2 (iter 24)</span></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="al">![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_024.png)</span>{.lightbox group="ucb-sp"}</span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block 3 (iter 36)</span></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a><span class="al">![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_036.png)</span>{.lightbox group="ucb-sp"}</span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Block 4 (iter 48)</span></span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a><span class="al">![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_048.png)</span>{.lightbox group="ucb-sp"}</span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next Steps: SGD Optimizer for W</span></span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a>The remaining unexplored dimension is the optimizer itself. A **separate SGD optimizer** (with momentum) for W may produce sharper gradients than Adam's adaptive learning rate, potentially enabling L1 to produce more exact zeros. This is the next planned code modification (block 5, iterations 53--56). However, given that the ceiling is now understood to be an identifiability limit rather than an optimization failure, expectations are low.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>NeuralGraph - Janelia Research Campus</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","openEffect":"zoom","loop":false,"descPosition":"bottom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>