---
title: "Case Study: Sparse Connectivity"
subtitle: "Breaking the 0.489 ceiling --- LLM-guided code modifications to the GNN architecture (32+ iterations)"
---

## Problem Statement

Sparse connectivity (filling_factor=50%, n=100 neurons) produces neural activity with subcritical spectral radius (rho=0.746). The landscape exploration (blocks 7, 8, 17) established that **no configuration sweep can break the conn_R^2^=0.489 ceiling** --- neither regularization, learning rates, epochs, noise, nor tripling the data (30k frames) had any effect. This motivated a dedicated exploration focused on **modifying the GNN code itself**.

The central question: **can code-level modifications to the GNN architecture break the 0.489 ceiling?**

::: {.callout-note}
## From Landscape to Dedicated Exploration
This case study builds upon the **188-iteration landscape exploration** ([Landscape Results](results.qmd)), which systematically mapped GNN training configurations across 29 regimes. Blocks 7, 8, and 17 of that exploration devoted 36 iterations to the sparse regime, exhausting the configuration parameter space and establishing the conn_R^2^=0.489 ceiling that motivated the pivot to direct code modifications documented here.
:::

---

## Why Code Changes?

The landscape exploration exhausted the configuration parameter space across 36 iterations in 3 blocks:

| Block | n_frames | eff_rank | Best conn | Degeneracy | Key |
|:-----:|:--------:|:--------:|:---------:|:----------:|:----|
| 7 | 10k | 21 | 0.466 | 12/12 | All configs give conn~0.489 |
| 8 (noise) | 10k | 91 | 0.490 | 0/12 | Noise inflates eff_rank; no rescue |
| 17 | **30k** | **13** | **0.436** | **12/12** | **eff_rank DROPS; 30k useless** |

The LLM identified the root cause as an **architectural degeneracy** in the GNN's dual-pathway design:

$$\frac{du}{dt} = \text{lin\_phi}(u, a) + W \odot \text{lin\_edge}(u, a)$$

The `lin_phi` pathway can predict dynamics perfectly (test_pearson ≈ 1.000) without requiring correct W. At subcritical spectral radius, the W-mediated signal is too weak to overcome this compensation. The degeneracy gap is ~0.51.

::: {.callout-note}
## From Config to Code

The dedicated exploration deliberately shifted from hyperparameter tuning to **direct GNN code modification**: changing initialization, optimization, regularization mechanics, and MLP architecture in the source files (`Signal_Propagation.py`, `graph_trainer.py`, `MLP.py`).
:::

---

## Config Baseline (Iterations 1--8)

Before modifying code, the exploration confirmed the ceiling across 5 orders of L1 magnitude, 2--12 epochs, edge_diff up to 50000, and aggressive multi-parameter combinations. **All 8 iterations produced conn_R^2^ = 0.489.** This confirmed the ceiling is not a training configuration problem, justifying the pivot to code changes.

---

## Code Modification 1: W Initialization Scale

**Iterations 9--12** · File: `Signal_Propagation.py`

**Hypothesis**: The true W matrix has entries ~ N(0, g/sqrt(n)) ≈ N(0, 0.7), but initialization uses N(0, 1.0) --- a scale mismatch that forces early training to waste capacity correcting the scale before learning structure.

**Code change**:
```python
# Before: W_init = torch.randn(n_neurons, n_neurons)
# After:
W_init = torch.randn(n_neurons, n_neurons) * (1.0 / math.sqrt(self.n_neurons))
```

**Results**: conn_R^2^ = 0.489--0.490 across all 4 config variations (lr_W 1E-3 to 3E-3, L1 1E-4 to 1E-3, n_epochs 2--12).

::: {.callout-warning}
### Verdict: Zero effect

Init scale is NOT the bottleneck. The optimizer corrects scale quickly; the problem is structural.
:::

---

## Code Modification 2: Proximal L1 Soft-Thresholding

**Iterations 13--16** · File: `graph_trainer.py`

**Hypothesis**: Standard gradient-based L1 regularization penalizes W entries but never creates exact zeros. Since the true sparse W has 50% zeros, **proximal soft-thresholding** after each optimizer step should force exact sparsity, potentially aligning the learned structure with the true connectivity pattern.

**Code change**:
```python
# After optimizer.step():
with torch.no_grad():
    threshold = coeff_W_L1 * lr_W
    W = model.W
    W.data = torch.sign(W.data) * torch.clamp(
        torch.abs(W.data) - threshold, min=0
    )
```

**Results**:

| Iter | L1 | Threshold | conn_R^2^ | Effect |
|:----:|:--:|:---------:|:---------:|:------:|
| 13 | 1E-4 | 3E-7 | 0.490 | none --- threshold negligible vs W scale (~0.1) |
| 14 | 1E-3 | 3E-6 | 0.488 | none --- still too small |
| 15 | **1E-2** | **3E-5** | **0.361** | **destructive** --- training collapses |
| 16 | 1E-3 | 3E-6 (no warm-up) | 0.488 | none --- warm-up phase irrelevant |

::: {.callout-warning}
### Verdict: Zero effect at moderate; destructive at extreme

The fundamental problem: proximal threshold = L1 × lr_W produces values 3--4 orders below the W entry scale (~0.1). To reach effective thresholds, L1 must be so large that it destroys the loss landscape. **The sparsity problem cannot be solved by regularization mechanics alone.**
:::

---

## Code Modification 3: MLP Capacity Reduction

**Iterations 17--20** · Files: `Signal_Propagation.py`, `MLP.py`

**Hypothesis**: If `lin_phi` (the bypass pathway) has too much capacity, it absorbs all predictive power and leaves W unconstrained. **Reducing MLP width or depth** should force the model to route signal through W, breaking the degeneracy.

**Code changes**: Reduced `hidden_dim` (64→32→16) and `n_layers` (3→2) in the MLP configuration.

**Results**:

| Iter | Change | conn_R^2^ | test_pearson | Effect |
|:----:|--------|:---------:|:------------:|:------:|
| 17 | hidden_dim 64→32 | 0.489 | 0.999 | none --- MLP still compensates |
| 18 | hidden_dim 32 + L1=1E-3 | 0.488 | 0.999 | none --- combined still fails |
| 19 | **n_layers 3→2** | **0.010** | **-0.035** | **catastrophic failure** |
| 20 | **hidden_dim 64→16** | **0.017** | **-0.072** | **catastrophic failure** |

::: {.callout-important}
### Established Principle: MLP has a hard minimum capacity

3 layers and hidden_dim≥32 are structurally necessary for the GNN to function at all. Below these thresholds, the model cannot even predict dynamics. Above the minimum, the MLP **always** fully compensates for incorrect W. There is no "Goldilocks zone" where MLP capacity is enough for dynamics but insufficient for W bypass.
:::

---

## Code Modification 4: Gradient Clipping on W

**Iterations 21--24** · File: `graph_trainer.py`

**Hypothesis**: W has n^2^=10,000 entries receiving gradients from all neuron pairs. Large, noisy gradients may destabilize W learning, preventing convergence to the true sparse pattern. **Gradient clipping** should stabilize the W update trajectory.

**Code change**:
```python
# Before optimizer.step():
torch.nn.utils.clip_grad_norm_([model.W], max_norm=1.0)
```

**Results**: conn_R^2^ = 0.488--0.489 across all 4 config variations (baseline, L1=1E-3, lr_W=5E-3, n_epochs=12).

::: {.callout-warning}
### Verdict: Zero effect

Gradient noise is not the bottleneck. The W gradients carry correct directional information --- the problem is that the loss landscape does not require W to be correct.
:::

---

## Code Modification 5: lin_phi Scaling / Removal

**Iterations 25--28** · File: `Signal_Propagation.py`, `graph_trainer.py`

**Hypothesis**: The dual-pathway architecture `du/dt = lin_phi(u,a) + W @ lin_edge(u,a)` allows `lin_phi` to bypass W entirely. By multiplying `lin_phi` output by a scaling factor (0.0--0.5), or removing it completely, the model should be forced to route signal through the `W @ lin_edge` pathway, making correct W recovery essential.

**Code change**:
```python
# Signal_Propagation.py: multiply lin_phi output by phi_scale
phi_output = self.phi_scale * self.lin_phi(x, a)
du_dt = phi_output + W @ self.lin_edge(x, a)
```

**Results**:

| Iter | phi_scale | L1 | conn_R^2^ | test_pearson | Gap | Effect |
|:----:|:---------:|:--:|:---------:|:------------:|:---:|:------:|
| 25 | 0.1 | 1E-4 | 0.489 | 0.9997 | 0.511 | none |
| 26 | 0.5 | 1E-4 | 0.490 | 0.9996 | 0.511 | none |
| 27 | **0.0** | 1E-4 | 0.489 | 0.9994 | 0.510 | **none --- complete removal** |
| 28 | 0.1 | 1E-3 | 0.488 | 0.9986 | 0.511 | none --- L1 still irrelevant |

::: {.callout-important}
### Established Principle: lin_phi is NOT the degeneracy source

Even **complete removal** of lin_phi (phi_scale=0.0, so `du/dt = W @ lin_edge(u,a)` only) produces conn_R^2^=0.489. The degeneracy is entirely within **lin_edge**: the flexible MLP can absorb any W, so `W_wrong @ f(u,a) ≈ W_true @ g(u,a)` regardless of whether lin_phi exists. This falsifies the initial architectural hypothesis and redirects the diagnosis from the bypass pathway to the message-passing MLP itself.
:::

---

## Summary: What Failed and Why

| Code Modification | Hypothesis | Outcome | Reason |
|---|---|---|---|
| W init scale (1/sqrt(n)) | Scale mismatch wastes early training | Zero effect | Optimizer corrects scale quickly |
| Proximal L1 soft-thresholding | Exact zeros needed for sparsity | Zero/destructive | Threshold too small vs W scale |
| MLP hidden_dim reduction | Force signal through W pathway | Zero/catastrophic | No Goldilocks zone exists |
| Gradient clipping on W | Stabilize W trajectory | Zero effect | Gradients are directionally correct |
| **lin_phi scaling/removal** | **Bypass pathway absorbs signal** | **Zero effect** | **Degeneracy is in lin_edge, not lin_phi** |

::: {.callout-warning}
## Key Result

**All 28 iterations produced conn_R^2^ = 0.489** (±0.001, excluding catastrophic failures from aggressive MLP reduction). Five distinct code modifications --- targeting initialization, regularization mechanics, model capacity, optimization stability, and the lin_phi bypass pathway --- all failed. The architectural degeneracy is fundamental and resides in the lin_edge MLP's ability to compensate for any W.
:::

---

## Architectural Diagnosis

The exploration converges on a refined diagnosis. The degeneracy is **not** in the dual-pathway bypass (lin_phi), but in the **lin_edge MLP's expressiveness**:

1. **W signal is weak**: subcritical spectral radius (rho=0.746) means W-mediated dynamics carry less information than in supercritical regimes
2. **lin_edge compensates for any W**: even with lin_phi completely removed (phi_scale=0.0), the flexible lin_edge MLP satisfies `W_wrong @ f(u,a) ≈ W_true @ g(u,a)` --- different W matrices paired with different lin_edge functions produce identical predictions
3. **The loss function cannot distinguish correct W**: at subcritical spectral radius, the W-mediated signal is weak enough that many (W, lin_edge) pairs achieve equivalent loss values
4. **Local code changes cannot break this degeneracy**: init scale, regularization mechanics, capacity reduction, gradient control, and lin_phi removal all operate downstream of the fundamental problem --- the product `W @ lin_edge(u,a)` is under-determined

This is qualitatively different from the dense regime (rho > 1.0) where W-mediated dynamics carry strong, unique signal that constrains the (W, lin_edge) pair to the correct solution.

---

## Next Steps: Recurrent Training and lin_edge Constraints

Since lin_phi removal had zero effect, the next interventions target the **lin_edge degeneracy** directly:

1. **Recurrent training (in progress)**: Multi-step rollout (time_step=4, 16, 32) should compound errors from wrong W over multiple prediction steps. Single-step training allows `W_wrong @ f(u,a) ≈ W_true @ g(u,a)` at each step independently; recurrent training penalizes wrong W because errors accumulate across steps. Planned with phi_scale=0.0 (simplifies model since lin_phi is confirmed irrelevant).

2. **lin_edge constraint**: Restrict lin_edge to a simpler function class (linear, or identity) so that it cannot compensate for wrong W. If `lin_edge(u,a) = u` (identity), then the product `W @ u` directly exposes W.

3. **Separate W optimizer**: Use SGD with momentum for W (sharper sparsity) while keeping Adam for MLPs.

4. **Cosine LR scheduler for W**: Large early W updates followed by fine-tuning.

These represent the next phase of the exploration --- moving from local code modifications to **constraining the lin_edge MLP** that is the true source of degeneracy.
