---
title: "Case Study: Sparse Connectivity"
subtitle: "Breaking the 0.489 ceiling --- LLM-guided code modifications to the GNN architecture (52 iterations)"
---

## Problem Statement

Sparse connectivity (filling_factor=50%, n=100 neurons) produces neural activity with subcritical spectral radius (rho=0.746). The landscape exploration (blocks 7, 8, 17) established that **no configuration sweep can break the conn_R^2^=0.489 ceiling** --- neither regularization, learning rates, epochs, noise, nor tripling the data (30k frames) had any effect. This motivated a dedicated exploration focused on **modifying the GNN code itself**.

The central question: **can code-level modifications to the GNN architecture break the 0.489 ceiling?** After 52 iterations testing 10 code modifications across 3 random seeds, the answer is **no** --- the ceiling is a fundamental identifiability limit, not an architectural one.

::: {.callout-note}
## From Landscape to Dedicated Exploration
This case study builds upon the **348-iteration landscape exploration** ([Landscape Results](results.qmd)), which systematically mapped GNN training configurations across 29 regimes. Blocks 7, 8, and 17 of that exploration devoted 36 iterations to the sparse regime, exhausting the configuration parameter space and establishing the conn_R^2^=0.489 ceiling that motivated the pivot to direct code modifications documented here.
:::

---

## Why Code Changes?

The landscape exploration exhausted the configuration parameter space across 36 iterations in 3 blocks:

| Block | n_frames | eff_rank | Best conn | Degeneracy | Key |
|:-----:|:--------:|:--------:|:---------:|:----------:|:----|
| 7 | 10k | 21 | 0.466 | 12/12 | All configs give conn~0.489 |
| 8 (noise) | 10k | 91 | 0.490 | 0/12 | Noise inflates eff_rank; no rescue |
| 17 | **30k** | **13** | **0.436** | **12/12** | **eff_rank DROPS; 30k useless** |

The LLM identified the root cause as an **architectural degeneracy** in the GNN's dual-pathway design:

$$\frac{du}{dt} = \text{lin\_phi}(u, a) + W \odot \text{lin\_edge}(u, a)$$

The `lin_phi` pathway can predict dynamics perfectly (test_pearson ≈ 1.000) without requiring correct W. At subcritical spectral radius, the W-mediated signal is too weak to overcome this compensation. The degeneracy gap is ~0.51.

::: {.callout-note}
## From Config to Code

The dedicated exploration deliberately shifted from hyperparameter tuning to **direct GNN code modification**: changing initialization, optimization, regularization mechanics, and MLP architecture in the source files (`Signal_Propagation.py`, `graph_trainer.py`, `MLP.py`).
:::

---

## Config Baseline (Iterations 1--8)

Before modifying code, the exploration confirmed the ceiling across 5 orders of L1 magnitude, 2--12 epochs, edge_diff up to 50000, and aggressive multi-parameter combinations. **All 8 iterations produced conn_R^2^ = 0.489.** This confirmed the ceiling is not a training configuration problem, justifying the pivot to code changes.

---

## Code Modification 1: W Initialization Scale

**Iterations 9--12** · File: `Signal_Propagation.py`

**Hypothesis**: The true W matrix has entries ~ N(0, g/sqrt(n)) ≈ N(0, 0.7), but initialization uses N(0, 1.0) --- a scale mismatch that forces early training to waste capacity correcting the scale before learning structure.

**Code change**:
```python
# Before: W_init = torch.randn(n_neurons, n_neurons)
# After:
W_init = torch.randn(n_neurons, n_neurons) * (1.0 / math.sqrt(self.n_neurons))
```

**Results**: conn_R^2^ = 0.489--0.490 across all 4 config variations (lr_W 1E-3 to 3E-3, L1 1E-4 to 1E-3, n_epochs 2--12).

::: {.callout-warning}
### Verdict: Zero effect

Init scale is NOT the bottleneck. The optimizer corrects scale quickly; the problem is structural.
:::

---

## Code Modification 2: Proximal L1 Soft-Thresholding

**Iterations 13--16** · File: `graph_trainer.py`

**Hypothesis**: Standard gradient-based L1 regularization penalizes W entries but never creates exact zeros. Since the true sparse W has 50% zeros, **proximal soft-thresholding** after each optimizer step should force exact sparsity, potentially aligning the learned structure with the true connectivity pattern.

**Code change**:
```python
# After optimizer.step():
with torch.no_grad():
    threshold = coeff_W_L1 * lr_W
    W = model.W
    W.data = torch.sign(W.data) * torch.clamp(
        torch.abs(W.data) - threshold, min=0
    )
```

**Results**:

| Iter | L1 | Threshold | conn_R^2^ | Effect |
|:----:|:--:|:---------:|:---------:|:------:|
| 13 | 1E-4 | 3E-7 | 0.490 | none --- threshold negligible vs W scale (~0.1) |
| 14 | 1E-3 | 3E-6 | 0.488 | none --- still too small |
| 15 | **1E-2** | **3E-5** | **0.361** | **destructive** --- training collapses |
| 16 | 1E-3 | 3E-6 (no warm-up) | 0.488 | none --- warm-up phase irrelevant |

::: {.callout-warning}
### Verdict: Zero effect at moderate; destructive at extreme

The fundamental problem: proximal threshold = L1 × lr_W produces values 3--4 orders below the W entry scale (~0.1). To reach effective thresholds, L1 must be so large that it destroys the loss landscape. **The sparsity problem cannot be solved by regularization mechanics alone.**
:::

---

## Code Modification 3: MLP Capacity Reduction

**Iterations 17--20** · Files: `Signal_Propagation.py`, `MLP.py`

**Hypothesis**: If `lin_phi` (the bypass pathway) has too much capacity, it absorbs all predictive power and leaves W unconstrained. **Reducing MLP width or depth** should force the model to route signal through W, breaking the degeneracy.

**Code changes**: Reduced `hidden_dim` (64→32→16) and `n_layers` (3→2) in the MLP configuration.

**Results**:

| Iter | Change | conn_R^2^ | test_pearson | Effect |
|:----:|--------|:---------:|:------------:|:------:|
| 17 | hidden_dim 64→32 | 0.489 | 0.999 | none --- MLP still compensates |
| 18 | hidden_dim 32 + L1=1E-3 | 0.488 | 0.999 | none --- combined still fails |
| 19 | **n_layers 3→2** | **0.010** | **-0.035** | **catastrophic failure** |
| 20 | **hidden_dim 64→16** | **0.017** | **-0.072** | **catastrophic failure** |

::: {.callout-important}
### Established Principle: MLP has a hard minimum capacity

3 layers and hidden_dim≥32 are structurally necessary for the GNN to function at all. Below these thresholds, the model cannot even predict dynamics. Above the minimum, the MLP **always** fully compensates for incorrect W. There is no "Goldilocks zone" where MLP capacity is enough for dynamics but insufficient for W bypass.
:::

---

## Code Modification 4: Gradient Clipping on W

**Iterations 21--24** · File: `graph_trainer.py`

**Hypothesis**: W has n^2^=10,000 entries receiving gradients from all neuron pairs. Large, noisy gradients may destabilize W learning, preventing convergence to the true sparse pattern. **Gradient clipping** should stabilize the W update trajectory.

**Code change**:
```python
# Before optimizer.step():
torch.nn.utils.clip_grad_norm_([model.W], max_norm=1.0)
```

**Results**: conn_R^2^ = 0.488--0.489 across all 4 config variations (baseline, L1=1E-3, lr_W=5E-3, n_epochs=12).

::: {.callout-warning}
### Verdict: Zero effect

Gradient noise is not the bottleneck. The W gradients carry correct directional information --- the problem is that the loss landscape does not require W to be correct.
:::

---

## Code Modification 5: lin_phi Scaling / Removal

**Iterations 25--28** · File: `Signal_Propagation.py`, `graph_trainer.py`

**Hypothesis**: The dual-pathway architecture `du/dt = lin_phi(u,a) + W @ lin_edge(u,a)` allows `lin_phi` to bypass W entirely. By multiplying `lin_phi` output by a scaling factor (0.0--0.5), or removing it completely, the model should be forced to route signal through the `W @ lin_edge` pathway, making correct W recovery essential.

**Code change**:
```python
# Signal_Propagation.py: multiply lin_phi output by phi_scale
phi_output = self.phi_scale * self.lin_phi(x, a)
du_dt = phi_output + W @ self.lin_edge(x, a)
```

**Results**:

| Iter | phi_scale | L1 | conn_R^2^ | test_pearson | Gap | Effect |
|:----:|:---------:|:--:|:---------:|:------------:|:---:|:------:|
| 25 | 0.1 | 1E-4 | 0.489 | 0.9997 | 0.511 | none |
| 26 | 0.5 | 1E-4 | 0.490 | 0.9996 | 0.511 | none |
| 27 | **0.0** | 1E-4 | 0.489 | 0.9994 | 0.510 | **none --- complete removal** |
| 28 | 0.1 | 1E-3 | 0.488 | 0.9986 | 0.511 | none --- L1 still irrelevant |

::: {.callout-important}
### Established Principle: lin_phi is NOT the degeneracy source

Even **complete removal** of lin_phi (phi_scale=0.0, so `du/dt = W @ lin_edge(u,a)` only) produces conn_R^2^=0.489. The degeneracy is entirely within **lin_edge**: the flexible MLP can absorb any W, so `W_wrong @ f(u,a) ≈ W_true @ g(u,a)` regardless of whether lin_phi exists. This falsifies the initial architectural hypothesis and redirects the diagnosis from the bypass pathway to the message-passing MLP itself.
:::

---

## Code Modification 6: Recurrent Training

**Iterations 29--32** · File: `graph_trainer.py`

**Hypothesis**: Multi-step rollout (time_step=4, 16, 32) should compound errors from wrong W over multiple prediction steps. Single-step training allows `W_wrong @ f(u,a) ≈ W_true @ g(u,a)` at each step independently; recurrent training penalizes wrong W because errors accumulate.

**Results**: conn_R^2^ = 0.489 at time_step=4 and 16; slight degradation at time_step=32. noise_recurrent_level>0.01 is destructive.

::: {.callout-warning}
### Verdict: Zero effect or destructive

Recurrent training does not help in the subcritical regime. Error compounding requires the W-mediated signal to be strong enough that wrong W produces detectable trajectory divergence --- at rho=0.746, the signal is too weak.
:::

---

## Code Modification 7: Anti-Sparsity Penalty on W

**Iterations 33--36** · File: `graph_trainer.py`

**Hypothesis**: If the optimizer is trapped in a degenerate W solution, an **anti-sparsity penalty** during phase 1 (penalizing W entries near zero) could force W to explore denser regions of parameter space before L1 refines sparsity in phase 2, disrupting the (W, lin_edge) equilibrium.

**Results**: conn_R^2^ drops to 0.023--0.181 across all configurations.

::: {.callout-warning}
### Verdict: Universally destructive

Anti-sparsity penalty destroys W recovery entirely. Forcing W away from zero during warm-up does not disrupt the equilibrium --- it creates a worse starting point that L1 cannot recover from.
:::

---

## Code Modification 8: Freeze lin_edge After Warm-Up

**Iterations 37--40** · File: `graph_trainer.py`

**Hypothesis**: If lin_edge adapts during training to compensate for wrong W, **freezing lin_edge after the warm-up phase** should force subsequent training to correct W rather than adjusting the MLP.

**Results**: conn_R^2^ = 0.489 across all 4 configurations.

::: {.callout-warning}
### Verdict: Zero effect

The degenerate lin_edge function is already learned during the warm-up phase. Freezing it afterward does not change W --- the compensation is baked in before the freeze takes effect.
:::

---

## Code Modification 9: lin_edge Dropout

**Iterations 41--44** · File: `graph_trainer.py`

**Hypothesis**: Applying dropout (p=0.3, 0.5) to lin_edge during training should randomly disable MLP units, preventing the MLP from developing a coherent compensation strategy and forcing W to carry more signal.

**Results**: conn_R^2^ = 0.489 at both p=0.3 and p=0.5.

::: {.callout-warning}
### Verdict: Zero effect

The MLP compensation is distributed and redundant --- dropout cannot disrupt it. The lin_edge function learns a robust compensation that survives random unit masking.
:::

---

## Code Modification 10: lin_edge Mode Bypass (Paradigm Shift)

**Iterations 45--48** · Files: `Signal_Propagation.py`, `config.py`

**Hypothesis**: If lin_edge MLP compensation is the degeneracy source, replacing it with a **fixed nonlinearity** (tanh or identity) should eliminate compensation entirely and force W to be correct.

**Code change**: Added `lin_edge_mode` parameter to select between MLP (default), fixed tanh, or identity for the edge message function.

**Results**:

| Iter | lin_edge_mode | conn_R^2^ | test_pearson | Gap | Effect |
|:----:|:------------:|:---------:|:------------:|:---:|:------:|
| 45 | tanh | **0.489** | 0.423 | 0.066 | **conn IDENTICAL; dynamics much worse** |
| 46 | tanh + L1=1E-3 | **0.489** | 0.424 | 0.065 | L1 still irrelevant at tanh |
| 47 | tanh + 12ep | **0.489** | 0.424 | 0.065 | more training does not help |
| 48 | **identity** | **0.009** | -0.035 | -- | **catastrophic failure** |

::: {.callout-important}
### Paradigm Shift: The Degeneracy Gap Was a Red Herring

Replacing the MLP with a **fixed tanh nonlinearity** gives **identical conn_R^2^=0.489** while test_pearson drops from 1.000 to 0.423. This fundamentally changes the diagnosis:

- The MLP was **not causing** the W recovery ceiling --- it was only **improving dynamics prediction** on top of it
- The degeneracy gap (test_pearson - conn_R^2^ ≈ 0.51) was a red herring: it reflected MLP compensation *improving dynamics*, not *preventing W recovery*
- With fixed tanh, the gap collapses to ~0.065 (both metrics are low), confirming that the ceiling is about **identifiability**, not compensation

The 0.489 ceiling is a **fundamental identifiability limit** of the model form `du/dt = W @ f(u,a)` at this data regime, not an architectural degeneracy.
:::

---

## Seed Universality Confirmation

**Iterations 49--52** · Seed variation (42, 137, 256)

To confirm the ceiling is not specific to one random W realization, three different seeds were tested:

| Iter | Seed | n_epochs | conn_R^2^ | test_pearson | Effect |
|:----:|:----:|:--------:|:---------:|:------------:|:------:|
| 49 | 42 | 6 | 0.489 | 0.423 | identical ceiling |
| 50 | 137 | 12 | 0.489 | 0.424 | 12 epochs = 6 epochs |
| 51 | 137 | 6 (lr_W=1E-2) | 0.489 | 0.425 | 3x lr_W = zero effect |
| 52 | 256 | 6 | 0.489 | 0.429 | identical ceiling |

::: {.callout-important}
### Established: The 0.489 Ceiling Is Universal

Three different random W realizations all produce conn_R^2^=0.489. The ceiling is a property of the model form + data regime, not of any specific connectivity matrix.
:::

---

## Summary: What Failed and Why

| Code Modification | Iters | Hypothesis | Outcome | Reason |
|---|:---:|---|---|---|
| W init scale (1/sqrt(n)) | 9--12 | Scale mismatch wastes early training | Zero effect | Optimizer corrects scale quickly |
| Proximal L1 soft-thresholding | 13--16 | Exact zeros needed for sparsity | Zero/destructive | Threshold too small vs W scale |
| MLP hidden_dim reduction | 17--20 | Force signal through W pathway | Zero/catastrophic | No Goldilocks zone exists |
| Gradient clipping on W | 21--24 | Stabilize W trajectory | Zero effect | Gradients are directionally correct |
| lin_phi scaling/removal | 25--28 | Bypass pathway absorbs signal | Zero effect | Degeneracy is in lin_edge, not lin_phi |
| Recurrent training | 29--32 | Compound errors from wrong W | Zero/destructive | W signal too weak for error compounding |
| Anti-sparsity penalty | 33--36 | Disrupt (W, lin_edge) equilibrium | Destructive | Creates worse starting point |
| Freeze lin_edge | 37--40 | Force W correction after warm-up | Zero effect | Degenerate function already baked in |
| lin_edge dropout | 41--44 | Prevent coherent compensation | Zero effect | Compensation is distributed/redundant |
| **lin_edge mode bypass** | **45--48** | **Eliminate MLP compensation** | **PARADIGM SHIFT** | **Ceiling is identifiability, not compensation** |

::: {.callout-warning}
## Key Result

**All 52 iterations produced conn_R^2^ = 0.489** (±0.001, excluding catastrophic failures from aggressive MLP reduction, anti-sparsity penalty, and identity lin_edge). Ten distinct code modifications --- targeting initialization, regularization mechanics, model capacity, optimization stability, the lin_phi bypass pathway, recurrent training, anti-sparsity, lin_edge freezing, lin_edge dropout, and lin_edge replacement --- all failed. The final paradigm shift (lin_edge_mode=tanh) revealed that the ceiling is a **fundamental identifiability limit**, not an architectural degeneracy. The degeneracy gap was a red herring.
:::

---

## Revised Architectural Diagnosis

The exploration converges on a **revised** diagnosis after the lin_edge_mode paradigm shift. The ceiling is **not** caused by MLP compensation --- it is a fundamental identifiability limit:

1. **W signal is weak**: subcritical spectral radius (rho=0.746) means W-mediated dynamics carry limited information
2. **The identifiability limit is model-form-dependent**: `du/dt = W @ f(u,a)` with a fixed f(u,a) = tanh(u) gives identical conn_R^2^=0.489 --- the product `W @ tanh(u)` cannot recover the true W from 10,000 frames of 100-neuron sparse activity regardless of the function form
3. **MLP compensation improves dynamics, not degrades W**: the MLP lin_edge lifts test_pearson from 0.42 to 1.00 without changing conn_R^2^; it makes *predictions better* while W remains equally wrong
4. **The ceiling is universal across seeds**: three different W realizations (seeds 42, 137, 256) all produce conn_R^2^=0.489

This is qualitatively different from the dense regime (rho > 1.0) where the W-mediated signal is strong enough to uniquely determine W from the dynamics.

---

## UCB Exploration Trees

::: {.panel-tabset}

#### Block 1 (iter 12)
![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_012.png){.lightbox group="ucb-sp"}

#### Block 2 (iter 24)
![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_024.png){.lightbox group="ucb-sp"}

#### Block 3 (iter 36)
![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_036.png){.lightbox group="ucb-sp"}

#### Block 4 (iter 48)
![](log/Claude_exploration/instruction_signal_sparse_parallel/exploration_tree/ucb_tree_iter_048.png){.lightbox group="ucb-sp"}

:::

---

## Next Steps: SGD Optimizer for W

The remaining unexplored dimension is the optimizer itself. A **separate SGD optimizer** (with momentum) for W may produce sharper gradients than Adam's adaptive learning rate, potentially enabling L1 to produce more exact zeros. This is the next planned code modification (block 5, iterations 53--56). However, given that the ceiling is now understood to be an identifiability limit rather than an optimization failure, expectations are low.
