---
title: "Case Study: Low-Rank Connectivity"
subtitle: "Recovering W from rank-20 neural dynamics --- 60+ iterations of dedicated LLM-guided exploration"
---

## Problem Statement

Low-rank connectivity (rank 20, n=100 neurons) produces neural activity with effective rank ~12 --- the hardest regime for connectivity recovery. With fewer distinguishable activity modes, many different connectivity matrices W can generate the same neuron traces. The GNN must discover the true W from severely under-determined data.

The central question: **can we find GNN training parameters that reliably recover W from low-rank dynamics?**

---

## What the Landscape Exploration Found (Block 2)

The general landscape exploration (Block 2, iterations 13--24) devoted 12 iterations to the low-rank regime and achieved **75% convergence** (9/12), with a breakthrough at iteration 21.

### Progression

```
Iter 13: lr_W=4E-3, L1=1E-5      → conn=0.999, test_R2=0.902 (dynamics poor)
Iter 18: lr_W=4E-3, L1=1E-6      → conn=1.000, test_R2=0.925 (improved!)
Iter 19: lr_W=3E-3, L1=1E-5      → conn=0.999, test_R2=0.943 (better lr_W)
Iter 21: lr_W=3E-3, L1=1E-6      → conn=0.993, test_R2=0.996 (BREAKTHROUGH)
```

### Key discovery

Reducing L1 from 1E-5 to 1E-6 was the critical enabler for dynamics recovery. Combined with lr_W=3E-3, this achieved chaotic-baseline-level performance (test_R^2^=0.996) despite eff_rank=12.

### All 12 iterations

| Iter | lr_W | L1 | Factorization | batch | conn_R^2^ | test_R^2^ | eff_rank | Status |
|:----:|:----:|:--:|:---:|:---:|:----:|:----:|:---:|---|
| 13 | 4E-3 | 1E-5 | False | 8 | 0.999 | 0.902 | 13 | converged |
| 14 | 5E-3 | 1E-5 | **True** | 8 | 0.899 | 0.854 | 13 | partial |
| 15 | 8E-3 | 1E-5 | **True** | 8 | 0.983 | 0.851 | 14 | converged |
| 16 | 2E-3 | 1E-5 | False | 8 | 0.992 | 0.774 | 14 | converged |
| 17 | 5E-3 | 1E-5 | False | 8 | 0.385 | 0.782 | ~14 | **failed** |
| 18 | 4E-3 | **1E-6** | False | 8 | 1.000 | 0.925 | ~14 | converged |
| 19 | 3E-3 | 1E-5 | False | 8 | 0.999 | 0.943 | ~14 | converged |
| 20 | 1.5E-3 | 1E-5 | False | 8 | 0.881 | 0.802 | ~14 | partial |
| **21** | **3E-3** | **1E-6** | **False** | **8** | **0.993** | **0.996** | **12** | **converged** |
| 22 | 3.5E-3 | 1E-6 | False | 8 | 0.999 | 0.886 | 13 | converged |
| 23 | 2.5E-3 | 1E-5 | False | 8 | 0.978 | 0.679 | 12 | converged |
| 24 | 4E-3 | 1E-6 | False | **16** | 0.989 | 0.997 | - | converged |

---

## Established Principles

Five principles were established or refined during the low-rank exploration:

::: {.callout-note}
### 1. L1=1E-6 is critical for low-rank dynamics
At L1=1E-5, connectivity converges (R^2^ > 0.99) but dynamics remain poor (test_R^2^ ~0.90). Reducing L1 to 1E-6 unlocks near-perfect dynamics (test_R^2^ = 0.996). The mechanism: excessive L1 penalizes small W entries that encode the low-rank structure, forcing the MLP to compensate.
:::

::: {.callout-note}
### 2. Factorization hurts in low-rank regime
`low_rank_factorization=True` (W = W_L @ W_R) underperforms direct W learning. At lr_W=5E-3 factorization gives conn_R^2^=0.899 vs 0.999 without. The GNN recovers W structure better when learning a full matrix and letting L1 induce sparsity.
:::

::: {.callout-note}
### 3. Optimal lr_W shifts downward (4E-3 → 3E-3)
Compared to chaotic baseline (lr_W=4E-3), the low-rank regime needs lower lr_W=3E-3. The lower effective rank provides less gradient signal, so smaller learning rate steps avoid overshooting.
:::

::: {.callout-note}
### 4. Convergence boundary shifts upward
The minimum lr_W for convergence rises from ~1.5E-3 (chaotic) to ~2E-3 (low-rank). Below 2E-3, connectivity still converges but dynamics degrade severely (test_R^2^ < 0.80).
:::

::: {.callout-note}
### 5. Batch size sensitivity depends on L1
batch_size=16 degrades quality at L1=1E-5 but is safe at L1=1E-6 (iter 24: test_R^2^=0.997). Lower L1 removes the batch size sensitivity observed in block 1.
:::

---

## Regime Characteristics

| Property | Low-rank | Chaotic (baseline) |
|---|:---:|:---:|
| connectivity_type | low_rank (r=20) | chaotic |
| n_neurons | 100 | 100 |
| effective rank (99% var) | 12--14 | 31--35 |
| spectral radius | 0.952 (subcritical) | 1.065 (supercritical) |
| convergence rate | 75% (9/12) | 100% (12/12) |
| best conn_R^2^ | 0.999 | 0.999 |
| best test_R^2^ | 0.997 | 0.996 |
| degeneracy risk | moderate (1/12) | low |

Despite eff_rank being 3x lower, the low-rank regime achieves comparable peak performance to chaotic --- but the parameter window is narrower and degeneracy is a risk at suboptimal settings.

---

## Connectivity and Activity

::: {layout-ncol=2}
![Connectivity matrix: ground truth vs learned W (block 2, best iteration)](log/Claude_exploration/instruction_signal_landscape_parallel/connectivity_matrix/block_002.png){.lightbox group="block2"}

![Neural activity: ground truth and GNN rollout (block 2)](log/Claude_exploration/instruction_signal_landscape_parallel/activity/block_002.png){.lightbox group="block2"}
:::

---

## Degeneracy Analysis

Only 1/12 iterations showed degeneracy (iter 17: conn_R^2^=0.385 despite test_pearson=0.836). The low degeneracy rate is notable given that low eff_rank is theoretically degeneracy-prone.

The key anti-degeneracy levers identified:

- **L1=1E-6** (not 1E-5): avoids penalizing small-but-real W entries
- **lr_W=3E-3**: allows W to converge before MLPs can compensate
- **coeff_edge_diff**: constrains MLP monotonicity, reducing compensation ability

---

## Open Questions → Dedicated Exploration

The landscape exploration allocated only 12 iterations to low-rank. Several questions remain:

1. **Robustness across seeds**: Does lr_W=3E-3 + L1=1E-6 work for all random W realizations, or only for seed=137?
2. **Two-phase training**: Can `n_epochs_init` (no L1 in early epochs) + stronger L1 in phase 2 improve further?
3. **coeff_edge_diff scaling**: Values of 10,000+ constrain MLP compensation --- how does this interact with L1?
4. **Training duration**: The landscape used 1 epoch / 20 augmentation loops. Does 2 epochs / 200 augmentation loops change the optimal parameters?

These questions motivate a **dedicated low-rank LLM exploration loop** using `GNN_LLM_parallel.py` with fixed simulation and training-parameter-only search space.

```bash
python GNN_LLM_parallel.py -o generate_train_test_plot_Claude_cluster signal_low_rank iterations=120
```

---

## Dedicated Low-Rank Exploration (60+ iterations)

The dedicated exploration fixed the simulation (low_rank, rank=20, n=100, 10k frames) and searched only training hyper-parameters using UCB tree search with 4 parallel slots per batch. **60+ iterations across 5 completed blocks** were completed, testing **7 different random seeds** (42, 137, 7, 99, 256, 314, 500).

::: {.callout-tip}
## Key Result

**Connectivity recovery is essentially solved** (conn_R^2^ > 0.999 in nearly all iterations). The exploration refined the universal recipe template: lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8, n_epochs=2. Only lr_W and L1 require per-seed tuning. The L1 landscape is a cliff, not a gradient: no useful intermediate values between 1E-6 and 1E-5.
:::

### Per-Seed Best Configurations

| Seed | lr_W | L1 | batch | n_ep | init | test_R^2^ | conn_R^2^ | Notes |
|:----:|:----:|:--:|:-----:|:----:|:----:|:---------:|:---------:|:------|
| 42 | 5E-3 | 1E-5 | 8 | 2 | 2 | **0.998** | 1.000 | Best overall; fragile to perturbations |
| 256 | 6E-3 | 1E-5 | 8 | 2 | 2 | **0.994** | 1.000 | Sharply peaked lr_W (±0.5E-3 width) |
| 99 | 5E-3 | 1E-6 | 8 | 2 | 2 | **0.992** | 1.000 | Unique L1 requirement |
| 500 | 4E-3 | 1E-6 | 8 | 2 | 2 | **0.990** | 1.000 | Dual optimization (lr_W + L1) |
| 314 | 5E-3 | 1E-5 | 8 | 2 | 2 | **0.990** | 1.000 | Default recipe works |
| 137 | 5E-3 | 1E-5 | 8 | 2 | 2 | **0.989** | 1.000 | Baseline; robust to perturbations |
| 7 | 5E-3 | 1E-5 | 8 | 3 | 2 | **0.985** | 1.000 | n_epochs=3 helps |

### Block-by-Block Progression

| Block | Iters | Focus | Best test_R^2^ | Key Finding |
|:-----:|:-----:|-------|:---------------:|:------------|
| 1 | 1--12 | lr_W, L1, lr sweep | 0.9994 | lr_W=5E-3 optimal; seed=42 identified as best performer |
| 2 | 13--24 | Cross-seed validation | 0.9994 | n_epochs=3 catastrophically hurts easy seeds (seed=42: -0.105) |
| 3 | 25--36 | L1 cross-seed testing | 0.9920 | L1=1E-6 transforms seed=99 but catastrophically breaks seed=137 |
| 4 | 37--48 | seed=256, seed=314 | **0.9940** | seed=256 breakthrough via lr_W=6E-3 (sharply peaked) |
| 5 | 49--60 | seed=500 optimization | **0.9900** | seed=500 rescued via lr_W=4E-3 + L1=1E-6 (0.880→0.990) |

### Connectivity and Activity (Dedicated Exploration)

::: {layout-ncol=2}
![Connectivity matrix (block 5, best iteration)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_matrix/block_005.png){.lightbox group="dedicated"}

![Neural activity (block 5)](log/Claude_exploration/instruction_signal_low_rank_parallel/activity/block_005.png){.lightbox group="dedicated"}
:::

### UCB Exploration Trees

::: {.panel-tabset}

#### Block 1 (iter 12)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_012.png){.lightbox group="ucb-lr"}

#### Block 2 (iter 24)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_024.png){.lightbox group="ucb-lr"}

#### Block 3 (iter 36)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_036.png){.lightbox group="ucb-lr"}

#### Block 4 (iter 48)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_048.png){.lightbox group="ucb-lr"}

#### Block 5 (iter 60)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_060.png){.lightbox group="ucb-lr"}

:::

### Connectivity Scatter (Selected Iterations)

::: {layout-ncol=4}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_001.png){.lightbox group="scatter-lr"}

![Iter 13 (node 13)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_013.png){.lightbox group="scatter-lr"}

![Iter 47 (best)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_047.png){.lightbox group="scatter-lr"}

![Iter 55 (seed=200)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_055.png){.lightbox group="scatter-lr"}
:::

### MLP Functions (Selected Iterations)

::: {layout-ncol=4}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_001_MLP.png){.lightbox group="mlp-lr"}

![Iter 13](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_013_MLP.png){.lightbox group="mlp-lr"}

![Iter 47](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_047_MLP.png){.lightbox group="mlp-lr"}

![Iter 55](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_055_MLP.png){.lightbox group="mlp-lr"}
:::

### Kinograph (Selected Iterations)

::: {layout-ncol=2}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/kinograph/iter_001.png){.lightbox group="kino-lr"}

![Iter 47 (best overall)](log/Claude_exploration/instruction_signal_low_rank_parallel/kinograph/iter_047.png){.lightbox group="kino-lr"}
:::

---

## Established Principles (Dedicated Exploration)

16 principles were established across 5 blocks. Key findings:

::: {.callout-important}
### Optimal lr_W is seed-dependent
Most seeds optimal at 5E-3, but seed=256 has exclusive peak at 6E-3 (±0.5E-3 width) and seed=500 requires 4E-3. Per-seed tuning closes the gap: all 7 seeds reach test_R^2^≥0.985 when optimally tuned.
:::

::: {.callout-important}
### L1 landscape is a cliff, not a gradient
L1=1E-5 is optimal for strong seeds (42, 137, 256, 314). Weak seeds (99, 500) require L1=1E-6. But L1=1E-6 at seed=137 causes catastrophic collapse (conn_R^2^=0.319). No useful intermediate values exist between 1E-6 and 1E-5.
:::

::: {.callout-note}
### Universal recipe template
lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8, n_epochs=2. Only lr_W (4E-3 to 6E-3) and L1 (1E-6 or 1E-5) require per-seed tuning.
:::

::: {.callout-note}
### Easy seeds are more fragile to perturbations
seed=42 (best test_R^2^=0.998) degrades catastrophically with edge_diff=20000 (-0.120), n_epochs=3 (-0.105), or lr_W=6E-3 (-0.222). seed=137 is robust to the same perturbations.
:::

::: {.callout-note}
### batch_size=8 is essential
batch=8 is universally optimal. batch=16 degrades performance, especially at weak settings. This reverses the earlier finding that batch=16 was optimal.
:::

::: {.callout-note}
### W recovery is trivial; dynamics recovery is hard
Nearly all configs achieve conn_R^2^≥0.999. The challenge is dynamics: test_R^2^ varies from 0.333 (catastrophic) to 0.998 depending on lr_W and L1 combination.
:::

---

## Answers to Open Questions

All four questions from the landscape exploration are now answered:

1. **Robustness across seeds**: Yes, with per-seed tuning. The recipe template (lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8) is universal across all 7 seeds. Only lr_W (4E-3 to 6E-3) and L1 (1E-6 or 1E-5) require per-seed optimization.

2. **Two-phase training**: Yes. n_epochs_init=2 (phase 1 without L1) is universally beneficial. It stabilizes W convergence before L1 refinement.

3. **coeff_edge_diff scaling**: 10,000 is the universal sweet spot. Both 5,000 and 20,000 degrade performance (20,000 is catastrophic for seed=42: -0.120).

4. **Training duration**: 2 epochs is optimal for most seeds. 3 epochs helps seed=7 but hurts easy seeds (seed=42: -0.105). When lr_W is correct, less training is better.
