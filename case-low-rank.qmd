---
title: "Case Study: Low-Rank Connectivity"
subtitle: "Recovering W from rank-20 neural dynamics --- 128 iterations of dedicated LLM-guided exploration"
---

## Problem Statement

Low-rank connectivity (rank 20, n=100 neurons) produces neural activity with effective rank ~12 --- the hardest regime for connectivity recovery. With fewer distinguishable activity modes, many different connectivity matrices W can generate the same neuron traces. The GNN must discover the true W from severely under-determined data.

The central question: **can we find GNN training parameters that reliably recover W from low-rank dynamics?**

::: {.callout-note}
## From Landscape to Dedicated Exploration
This case study builds upon the **348-iteration landscape exploration** ([Landscape Results](results.qmd)), which systematically mapped GNN training configurations across 29 regimes. Block 2 of that exploration devoted 12 iterations to the low-rank regime, identifying the critical lr_W/L1 levers and achieving the breakthrough configuration (test_R^2^=0.996) that became the starting point for the 128-iteration dedicated exploration documented here.
:::

---

## What the Landscape Exploration Found (Block 2)

The general landscape exploration (Block 2, iterations 13--24) devoted 12 iterations to the low-rank regime and achieved **75% convergence** (9/12), with a breakthrough at iteration 21.

### Progression toward the optimal configuration

The landscape search converged on the optimal configuration through systematic elimination:

```
Iter 13: lr_W=4E-3, L1=1E-5      → conn=0.999, test_R2=0.902 (connectivity OK, dynamics poor)
Iter 18: lr_W=4E-3, L1=1E-6      → conn=1.000, test_R2=0.925 (L1 reduction unlocks dynamics)
Iter 19: lr_W=3E-3, L1=1E-5      → conn=0.999, test_R2=0.943 (lr_W reduction helps more)
Iter 21: lr_W=3E-3, L1=1E-6      → conn=0.993, test_R2=0.996 (BREAKTHROUGH — both levers combined)
```

The two critical levers were identified: **L1 reduction** (1E-5 → 1E-6) and **lr_W reduction** (4E-3 → 3E-3). Both independently improve dynamics, and combining them achieves chaotic-baseline-level performance (test_R^2^=0.996) despite eff_rank=12.

### The optimal configuration and its limits

The best configuration from block 2 --- lr_W=3E-3, L1=1E-6, batch=8, no factorization --- achieves test_R^2^=0.996 and conn_R^2^=0.993. This is remarkable: low-rank connectivity with eff_rank=12 (3x lower than chaotic) matches the chaotic baseline. **Connectivity recovery is not the bottleneck** --- nearly all 12 iterations achieve conn_R^2^>0.99 regardless of configuration. The challenge is purely in dynamics recovery (test_R^2^), which is highly sensitive to the lr_W/L1 combination.

However, block 2 tested only **one random seed** (the default simulation seed). The dedicated exploration later revealed that the optimal lr_W and L1 are **seed-dependent**, and some W realizations are fundamentally harder. The configuration found here serves as a recipe template, not a universal solution.

### The seed problem: foreshadowing

Even within block 2, signs of seed sensitivity were visible: the single failed iteration (iter 17: lr_W=5E-3, conn_R^2^=0.385) showed that stepping slightly outside the optimal lr_W window can cause catastrophic W recovery failure. The dedicated exploration (88 iterations, 10 seeds) later confirmed this as a fundamental feature: ~20% of random W realizations are "hard seeds" where no configuration achieves good recovery.

### All 12 iterations

| Iter | lr_W | L1 | Factorization | batch | conn_R^2^ | test_R^2^ | eff_rank | Status |
|:----:|:----:|:--:|:---:|:---:|:----:|:----:|:---:|---|
| 13 | 4E-3 | 1E-5 | False | 8 | 0.999 | 0.902 | 13 | converged |
| 14 | 5E-3 | 1E-5 | **True** | 8 | 0.899 | 0.854 | 13 | partial |
| 15 | 8E-3 | 1E-5 | **True** | 8 | 0.983 | 0.851 | 14 | converged |
| 16 | 2E-3 | 1E-5 | False | 8 | 0.992 | 0.774 | 14 | converged |
| 17 | 5E-3 | 1E-5 | False | 8 | 0.385 | 0.782 | ~14 | **failed** |
| 18 | 4E-3 | **1E-6** | False | 8 | 1.000 | 0.925 | ~14 | converged |
| 19 | 3E-3 | 1E-5 | False | 8 | 0.999 | 0.943 | ~14 | converged |
| 20 | 1.5E-3 | 1E-5 | False | 8 | 0.881 | 0.802 | ~14 | partial |
| **21** | **3E-3** | **1E-6** | **False** | **8** | **0.993** | **0.996** | **12** | **converged** |
| 22 | 3.5E-3 | 1E-6 | False | 8 | 0.999 | 0.886 | 13 | converged |
| 23 | 2.5E-3 | 1E-5 | False | 8 | 0.978 | 0.679 | 12 | converged |
| 24 | 4E-3 | 1E-6 | False | **16** | 0.989 | 0.997 | - | converged |

---

## Established Principles

Five principles were established or refined during the low-rank exploration:

::: {.callout-note}
### 1. L1=1E-6 is critical for low-rank dynamics
At L1=1E-5, connectivity converges (R^2^ > 0.99) but dynamics remain poor (test_R^2^ ~0.90). Reducing L1 to 1E-6 unlocks near-perfect dynamics (test_R^2^ = 0.996). The mechanism: excessive L1 penalizes small W entries that encode the low-rank structure, forcing the MLP to compensate.
:::

::: {.callout-note}
### 2. Factorization hurts in low-rank regime
`low_rank_factorization=True` (W = W_L @ W_R) underperforms direct W learning. At lr_W=5E-3 factorization gives conn_R^2^=0.899 vs 0.999 without. The GNN recovers W structure better when learning a full matrix and letting L1 induce sparsity.
:::

::: {.callout-note}
### 3. Optimal lr_W shifts downward (4E-3 → 3E-3)
Compared to chaotic baseline (lr_W=4E-3), the low-rank regime needs lower lr_W=3E-3. The lower effective rank provides less gradient signal, so smaller learning rate steps avoid overshooting.
:::

::: {.callout-note}
### 4. Convergence boundary shifts upward
The minimum lr_W for convergence rises from ~1.5E-3 (chaotic) to ~2E-3 (low-rank). Below 2E-3, connectivity still converges but dynamics degrade severely (test_R^2^ < 0.80).
:::

::: {.callout-note}
### 5. Batch size sensitivity depends on L1
batch_size=16 degrades quality at L1=1E-5 but is safe at L1=1E-6 (iter 24: test_R^2^=0.997). Lower L1 removes the batch size sensitivity observed in block 1.
:::

---

## Regime Characteristics

| Property | Low-rank | Chaotic (baseline) |
|---|:---:|:---:|
| connectivity_type | low_rank (r=20) | chaotic |
| n_neurons | 100 | 100 |
| effective rank (99% var) | 12--14 | 31--35 |
| spectral radius | 0.952 (subcritical) | 1.065 (supercritical) |
| convergence rate | 75% (9/12) | 100% (12/12) |
| best conn_R^2^ | 0.999 | 0.999 |
| best test_R^2^ | 0.997 | 0.996 |
| degeneracy risk | moderate (1/12) | low |

Despite eff_rank being 3x lower, the low-rank regime achieves comparable peak performance to chaotic --- but the parameter window is narrower and degeneracy is a risk at suboptimal settings.

---

## Connectivity and Activity

::: {layout-ncol=2}
![Connectivity matrix: ground truth vs learned W (block 2, best iteration)](log/Claude_exploration/instruction_signal_landscape_parallel/connectivity_matrix/block_002.png){.lightbox group="block2"}

![Neural activity: ground truth and GNN rollout (block 2)](log/Claude_exploration/instruction_signal_landscape_parallel/activity/block_002.png){.lightbox group="block2"}
:::

---

## Degeneracy Analysis

Only 1/12 iterations showed degeneracy (iter 17: conn_R^2^=0.385 despite test_pearson=0.836). The low degeneracy rate is notable given that low eff_rank is theoretically degeneracy-prone.

The key anti-degeneracy levers identified:

- **L1=1E-6** (not 1E-5): avoids penalizing small-but-real W entries
- **lr_W=3E-3**: allows W to converge before MLPs can compensate
- **coeff_edge_diff**: constrains MLP monotonicity, reducing compensation ability

---

## From Landscape to Dedicated Exploration

The landscape exploration identified the optimal lr_W/L1 combination for a single seed but left critical questions unanswered: does this generalize across W realizations? How do two-phase training, edge_diff, and epoch count interact? These questions motivated a **dedicated low-rank LLM exploration loop** --- 128 iterations across 14 random seeds, using UCB tree search with 4 parallel slots per batch.

```bash
python GNN_LLM_parallel.py -o generate_train_test_plot_Claude_cluster signal_low_rank iterations=120
```

---

## Dedicated Low-Rank Exploration (88 iterations)

The dedicated exploration fixed the simulation (low_rank, rank=20, n=100, 10k frames) and searched only training hyper-parameters using UCB tree search with 4 parallel slots per batch. **128 iterations across 11 completed blocks** were completed, testing **14 different random seeds** (42, 137, 7, 99, 256, 314, 500, 1000, 2000, 3000, 5000, 4000, 6000, 7000).

::: {.callout-tip}
## Key Result

**Connectivity recovery is essentially solved for 11 of 14 seeds** (conn_R^2^ > 0.999 in nearly all iterations). The exploration refined the universal recipe template: lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8, n_epochs=2. Only lr_W, L1, and occasionally coeff_edge_diff require per-seed tuning, with n_epochs=3 as an additional lever for mid-tier seeds. The L1 landscape is a cliff, not a gradient: no useful intermediate values between 1E-6 and 1E-5. Three seeds (1000, 3000, 5000) are irredeemable (~21% hard-seed rate). coeff_edge_diff=20000 emerges as a lever for mid-tier seeds (4000, 7000) in the extended exploration.
:::

### Per-Seed Best Configurations

| Seed | lr_W | L1 | edge_diff | batch | n_ep | init | test_R^2^ | conn_R^2^ | Notes |
|:----:|:----:|:--:|:---------:|:-----:|:----:|:----:|:---------:|:---------:|:------|
| 42 | 5E-3 | 1E-5 | 10000 | 8 | 2 | 2 | **0.998** | 1.000 | Best overall; fragile to perturbations |
| 500 | 4E-3 | 1E-6 | 15000 | 8 | 2 | 2 | **0.994** | 1.000 | Rescued via lr_W + L1 + edge_diff tuning |
| 256 | 6E-3 | 1E-5 | 10000 | 8 | 2 | 2 | **0.994** | 1.000 | Sharply peaked lr_W (±0.5E-3 width) |
| 99 | 5E-3 | 1E-6 | 10000 | 8 | 2 | 2 | **0.992** | 1.000 | Unique L1 requirement |
| 6000 | 5E-3 | 1E-5 | 10000 | 8 | 3 | 2 | **0.991** | 1.000 | n_epochs=3 transforms (like seed=2000) |
| 2000 | 5E-3 | 1E-5 | 10000 | 8 | **3** | 2 | **0.991** | 1.000 | n_epochs=3 transforms recovery (+0.073); L1=1E-6 hurts |
| 314 | 5E-3 | 1E-5 | 10000 | 8 | 2 | 2 | **0.990** | 1.000 | Default recipe works |
| 137 | 5E-3 | 1E-5 | 10000 | 8 | 2 | 2 | **0.989** | 1.000 | Baseline; robust to perturbations |
| 4000 | 5E-3 | 1E-6 | 20000 | 8 | 3 | 2 | **0.987** | 1.000 | Rescued via L1=1E-6 + edge_diff=20000 + 3ep triple combo |
| 7 | 5E-3 | 1E-5 | 10000 | 8 | 3 | 2 | **0.985** | 1.000 | n_epochs=3 helps |
| 7000 | 5E-3 | 1E-6 | 20000 | 8 | 2 | 2 | **0.974** | 1.000 | edge_diff=20000 + L1=1E-6 best; edge_diff=25000 overshoots |
| 5000 | 5E-3 | 1E-5 | 10000 | 8 | 2 | 2 | **0.953** | 1.000 | **Irredeemable-to-tune** --- 6 perturbations catastrophic; only standard recipe works |
| 3000 | 5E-3 | 1E-5 | 10000 | 8 | 3 | 2 | **0.597** | 0.322 | **Irredeemable** --- V_R^2^≈0.42, same pattern as seed=1000 |
| 1000 | 4E-3 | 1E-6 | 10000 | 8 | 2 | 2 | **0.516** | 0.330 | **Irredeemable** --- 15 attempts, all failed |

### Block-by-Block Progression

| Block | Iters | Focus | Best test_R^2^ | Key Finding |
|:-----:|:-----:|-------|:---------------:|:------------|
| 1 | 1--12 | lr_W, L1, lr sweep | 0.9994 | lr_W=5E-3 optimal; seed=42 identified as best performer |
| 2 | 13--24 | Cross-seed validation | 0.9994 | n_epochs=3 catastrophically hurts easy seeds (seed=42: -0.105) |
| 3 | 25--36 | L1 cross-seed testing | 0.9920 | L1=1E-6 transforms seed=99 but catastrophically breaks seed=137 |
| 4 | 37--48 | seed=256, seed=314 | **0.9940** | seed=256 breakthrough via lr_W=6E-3 (sharply peaked) |
| 5 | 49--60 | seed=500 optimization | **0.9900** | seed=500 rescued via lr_W=4E-3 + L1=1E-6 (0.880→0.990) |
| 6 | 61--72 | seed=500 refinement, seed=1000 | **0.9940** | seed=500 peak 0.994 (edge_diff=15000); seed=1000 catastrophic (6 attempts) |
| 7 | 73--80 | seed=1000 rescue, seed=2000 | **0.9180** | seed=1000 irredeemable after 12 total attempts; seed=2000 mid-tier (0.918) |
| 8 | 81--88 | seed=2000 optimization, new seeds | **0.9910** | seed=2000 transformed via n_epochs=3 (+0.073); seed=3000 catastrophic (0.406); combo mutations cause negative synergy |
| 9 | 89--96 | seed=5000, seed=4000, seed=6000 | **0.9670** | seed=5000 fragile (0.953 only at standard recipe, 6 perturbations catastrophic); seed=4000 mid-tier (0.967); seed=6000 transformed via n_epochs=3 (0.991) |
| 10 | 97--108 | seed=6000 tuning, seed=7000 discovery | **0.9910** | seed=6000 peak 0.991 (n_epochs=3); seed=7000 discovered (0.866 at standard, needs edge_diff tuning) |
| 11 | 121--128 | seed=7000 rescue, seed=4000 refinement | **0.9870** | seed=7000 rescued via edge_diff=20000+L1=1E-6 (0.974); seed=4000 peak 0.987 via triple combo; seed=5000 ABANDONED; edge_diff=25000 overshoots at seed=7000 |

### Connectivity and Activity (Dedicated Exploration)

::: {layout-ncol=2}
![Connectivity matrix (block 5, best iteration)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_matrix/block_005.png){.lightbox group="dedicated"}

![Neural activity (block 5)](log/Claude_exploration/instruction_signal_low_rank_parallel/activity/block_005.png){.lightbox group="dedicated"}
:::

### UCB Exploration Trees

::: {.panel-tabset}

#### Block 1 (iter 12)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_012.png){.lightbox group="ucb-lr"}

#### Block 2 (iter 24)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_024.png){.lightbox group="ucb-lr"}

#### Block 3 (iter 36)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_036.png){.lightbox group="ucb-lr"}

#### Block 4 (iter 48)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_048.png){.lightbox group="ucb-lr"}

#### Block 5 (iter 60)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_060.png){.lightbox group="ucb-lr"}

#### Block 6 (iter 72)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_072.png){.lightbox group="ucb-lr"}

#### Block 7 (iter 84)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_084.png){.lightbox group="ucb-lr"}

#### Block 8 (iter 96)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_096.png){.lightbox group="ucb-lr"}

#### Block 9 (iter 108)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_108.png){.lightbox group="ucb-lr"}

#### Block 10 (iter 120)
![](log/Claude_exploration/instruction_signal_low_rank_parallel/exploration_tree/ucb_tree_iter_120.png){.lightbox group="ucb-lr"}

:::

### Connectivity Scatter (Selected Iterations)

::: {layout-ncol=4}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_001.png){.lightbox group="scatter-lr"}

![Iter 13 (node 13)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_013.png){.lightbox group="scatter-lr"}

![Iter 47 (best)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_047.png){.lightbox group="scatter-lr"}

![Iter 55 (seed=200)](log/Claude_exploration/instruction_signal_low_rank_parallel/connectivity_scatter/iter_055.png){.lightbox group="scatter-lr"}
:::

### MLP Functions (Selected Iterations)

::: {layout-ncol=4}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_001_MLP.png){.lightbox group="mlp-lr"}

![Iter 13](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_013_MLP.png){.lightbox group="mlp-lr"}

![Iter 47](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_047_MLP.png){.lightbox group="mlp-lr"}

![Iter 55](log/Claude_exploration/instruction_signal_low_rank_parallel/mlp/iter_055_MLP.png){.lightbox group="mlp-lr"}
:::

### Kinograph (Selected Iterations)

::: {layout-ncol=2}
![Iter 1](log/Claude_exploration/instruction_signal_low_rank_parallel/kinograph/iter_001.png){.lightbox group="kino-lr"}

![Iter 47 (best overall)](log/Claude_exploration/instruction_signal_low_rank_parallel/kinograph/iter_047.png){.lightbox group="kino-lr"}
:::

---

## Established Principles (Dedicated Exploration)

18 principles were established across 11 blocks. Key findings:

::: {.callout-important}
### Optimal lr_W is seed-dependent
Most seeds optimal at 5E-3, but seed=256 has exclusive peak at 6E-3 (±0.5E-3 width), seed=500 requires 4E-3, and seed=2000 has a monotonically increasing landscape (4E-3→0.962, 5E-3→0.918, 6E-3→0.976, 7E-3→0.983 at n_epochs=2). Per-seed tuning closes the gap: 8 of 10 seeds reach test_R^2^≥0.985 when optimally tuned.
:::

::: {.callout-important}
### L1 landscape is a cliff, not a gradient
L1=1E-5 is optimal for strong seeds (42, 137, 256, 314). Weak seeds (99, 500) require L1=1E-6. But L1=1E-6 at seed=137 causes catastrophic collapse (conn_R^2^=0.319). No useful intermediate values exist between 1E-6 and 1E-5.
:::

::: {.callout-note}
### Universal recipe template
lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8, n_epochs=2. Only lr_W (4E-3 to 6E-3) and L1 (1E-6 or 1E-5) require per-seed tuning.
:::

::: {.callout-note}
### Easy seeds are more fragile to perturbations
seed=42 (best test_R^2^=0.998) degrades catastrophically with edge_diff=20000 (-0.120), n_epochs=3 (-0.105), or lr_W=6E-3 (-0.222). seed=137 is robust to the same perturbations.
:::

::: {.callout-note}
### batch_size=8 is essential
batch=8 is universally optimal. batch=16 degrades performance, especially at weak settings. This reverses the earlier finding that batch=16 was optimal.
:::

::: {.callout-note}
### W recovery is trivial; dynamics recovery is hard
Nearly all configs achieve conn_R^2^≥0.999. The challenge is dynamics: test_R^2^ varies from 0.333 (catastrophic) to 0.998 depending on lr_W and L1 combination.
:::

::: {.callout-important}
### coeff_edge_diff landscape is non-monotonic and seed-dependent
The standard value of 10,000 is the safe default. seed=500 benefits from edge_diff=15,000 (test_R^2^=0.994 vs 0.990), but edge_diff=12,500 creates a trough (0.907) and edge_diff=20,000 degrades (0.946). The extended exploration revealed that **edge_diff=20,000 is a powerful lever for mid-tier seeds**: seed=7000 improves from 0.866 to 0.974, seed=4000 from 0.967 to 0.987 (with L1=1E-6 + 3ep). However, edge_diff=25,000 overshoots at seed=7000 (0.895), and higher edge_diff + more epochs can overregularize (seed=7000: edge_diff=20000+3ep=0.921 vs 2ep=0.974). For easy seeds, edge_diff>10,000 generally hurts.
:::

::: {.callout-warning}
### ~21% of seeds are irredeemable or fragile
Three seeds (1000, 3000, 5000) catastrophically fail or are untuneable. seed=1000: best test_R^2^=0.516, conn_R^2^=0.330 after 15 configs (lr_W sweep 1E-3 to 7E-3, L1=1E-6, edge_diff=15,000, n_epochs=3, n_epochs_init=0, lr=5E-5/2E-4, recurrent training). seed=3000: test_R^2^=0.597, conn_R^2^=0.322 --- same V-factor bottleneck pattern (V_R^2^≈0.42). Both share a common signature: the V subspace remains stuck at ~0.41--0.43 regardless of configuration. seed=5000 represents a new failure mode: **fragile tunability** --- it achieves 0.953 at the exact standard recipe but all 6 tested perturbations (lr_W±1E-3, n_epochs=3, L1=1E-6, edge_diff=15000) produce catastrophic degradation (0.282--0.429). The seed has a single narrow peak that cannot be broadened.
:::

::: {.callout-note}
### L1 sensitivity is not simply "weak seeds benefit from 1E-6"
seed=99 and seed=500 benefit from L1=1E-6, but seed=2000 is hurt by L1=1E-6 (-0.090). seed=2000 behaves like strong seeds for L1 preference despite being mid-tier at baseline. The discriminator for L1 sensitivity is unknown --- it is not simply test_R^2^ magnitude.
:::

::: {.callout-important}
### n_epochs=3 transforms mid-tier seeds
seed=2000 jumps from 0.918 to 0.991 (+0.073) with n_epochs=3 at lr_W=5E-3. seed=7 similarly improves (0.975→0.985). However, easy seeds are hurt: seed=42 drops from 0.998 to 0.893. The pattern: mid-tier seeds at L1=1E-5 benefit most from the extra epoch.
:::

::: {.callout-warning}
### Combo mutations cause negative synergy
Combining two individually beneficial changes can produce worse results than either alone. seed=2000: n_epochs=3 alone gives 0.991, lr_W=6E-3 alone gives 0.976, but n_epochs=3 + lr_W=6E-3 gives only 0.956. The mechanism: higher lr_W with more epochs causes overtraining.
:::

---

## The Influence of the Training Seed

The training seed determines the random connectivity matrix W that the GNN must recover. Different W realizations produce dramatically different recovery difficulty, even though all share the same rank-20, n=100 structure. This is the single most important factor for dynamics recovery quality.

### Seed Difficulty Spectrum

The 14 tested seeds span a wide range:

| Tier | Seeds | test_R^2^ range | Characteristics |
|------|-------|:---------------:|:----------------|
| Strong | 42, 256, 500 | 0.994--0.998 | Require per-seed lr_W/L1 tuning but achieve near-perfect recovery |
| Good | 99, 6000, 2000, 314, 137 | 0.989--0.992 | Standard recipe works (137), needs L1=1E-6 (99), or n_epochs=3 (2000, 6000) |
| Moderate | 4000, 7 | 0.985--0.987 | Needs enhanced recipe: n_epochs=3 (7), or edge_diff=20000 + L1=1E-6 + 3ep triple combo (4000) |
| Low-moderate | 7000 | 0.974 | Needs edge_diff=20000 + L1=1E-6; edge_diff=25000 overshoots; edge_diff peak localized |
| Fragile | 5000 | 0.953 | Works only at exact standard recipe; 6 perturbations catastrophic; ABANDONED |
| Catastrophic | 1000, 3000 | 0.406--0.597 | Irredeemable --- V-factor bottleneck (V_R^2^≈0.42) |

### What Makes a Seed Hard?

The seed initializes the true connectivity matrix W. For the low-rank regime, W is constructed as a rank-20 matrix (W = U @ V where U, V are random Gaussian). Different random realizations produce W matrices with different spectral properties, condition numbers, and singular value distributions --- even though all are nominally rank 20.

The key diagnostic is the V subspace recovery: for hard seeds (1000 and 3000), V_R^2^ is stuck at ~0.41--0.43 regardless of training configuration, suggesting the GNN cannot learn the correct postsynaptic subspace. For all other seeds, V_R^2^ reaches 0.97+. Both hard seeds share this V-factor bottleneck signature, pointing to W realizations whose low-rank structure is particularly ill-conditioned for recovery from the observed dynamics. seed=1000 was tested across 15 configurations (lr_W sweep, L1 variants, epoch counts, recurrent training) --- all failed. seed=3000 immediately fails at the standard recipe with the same pattern. A third category emerged: seed=5000 achieves 0.953 at the standard recipe but is **fragile** --- all 6 tested perturbations (lr_W=4E-3, lr_W=6E-3, n_epochs=3, L1=1E-6, edge_diff=15000) produce catastrophic degradation, suggesting a narrow peak in parameter space that cannot be broadened.

### Practical Implication

Per-seed tuning closes the gap for 11/14 seeds (all reaching test_R^2^≥0.953), but ~21% of random W realizations (3/14 in this sample) are fundamentally hard or fragile. The universal recipe (lr=1E-4, coeff_edge_diff=10000, n_epochs_init=2, batch=8, n_epochs=2) with lr_W=5E-3 and L1=1E-5 works for most seeds, but optimization levers include: L1=1E-6 (seeds 99, 500, 4000, 7000), different lr_W (seed=256: 6E-3, seed=500: 4E-3), n_epochs=3 (seeds 7, 2000, 4000, 6000), or higher edge_diff (seed=500: 15000, seeds 4000/7000: 20000). The extended exploration revealed that **coeff_edge_diff=20000** is a powerful lever for mid-tier seeds, but non-monotonic: edge_diff=25000 overshoots at seed=7000 (0.895 vs 0.974 at 20000). Importantly, combining multiple levers can cause negative synergy (n_epochs=3 + lr_W=6E-3 at seed=2000 is worse than either alone; n_epochs=3 + edge_diff=20000 overregularizes at seed=7000).

---

## Answers to Open Questions

All four questions from the landscape exploration are now answered, plus new findings from the extended exploration:

1. **Robustness across seeds**: Mostly yes, with per-seed tuning. The recipe template (lr=1E-4, n_epochs_init=2, batch=8) is universal across 11/14 seeds. Only lr_W (4E-3 to 6E-3), L1 (1E-6 or 1E-5), n_epochs (2 or 3), and coeff_edge_diff (10000--20000) require per-seed optimization. Three seeds (1000, 3000, 5000) are irredeemable or fragile (~21% hard-seed rate).

2. **Two-phase training**: Yes. n_epochs_init=2 (phase 1 without L1) is universally beneficial. It stabilizes W convergence before L1 refinement. Eliminating phase 1 (n_epochs_init=3 with n_epochs=3) hurts.

3. **coeff_edge_diff scaling**: 10,000 is the safe default. The landscape is non-monotonic: seed=500 benefits from 15,000 (+0.004) but 12,500 creates a trough and 20,000 degrades. For other seeds, 15,000 generally hurts.

4. **Training duration**: 2 epochs is optimal for most seeds. However, n_epochs=3 is a powerful lever for mid-tier seeds: seed=2000 jumps from 0.918 to 0.991 (+0.073), seed=7 from 0.975 to 0.985. Easy seeds are hurt: seed=42 drops from 0.998 to 0.893. Combining n_epochs=3 with higher lr_W causes negative synergy.

5. **Seed irredeemability**: ~21% of W realizations (seeds 1000, 3000, 5000) are catastrophically hard or fragile. Seeds 1000 and 3000 share a V-factor bottleneck (V_R^2^≈0.42) regardless of training configuration --- the GNN cannot learn the correct postsynaptic subspace. seed=1000 was tested across 15 configurations, seed=3000 fails immediately at standard recipe with the same pattern. seed=5000 represents a new failure mode: it achieves 0.953 at the exact standard recipe but all perturbations are catastrophic.

6. **coeff_edge_diff=20000 as a lever for mid-tier seeds**: The extended exploration (blocks 9--11) discovered that higher edge_diff (20000 vs 10000) substantially helps seeds that plateau in the 0.86--0.97 range. seed=7000 improves from 0.866 to 0.974, seed=4000 from 0.967 to 0.987. However, the edge_diff landscape has a seed-specific peak: edge_diff=25000 overshoots at seed=7000, and combining high edge_diff with n_epochs=3 can overregularize.
