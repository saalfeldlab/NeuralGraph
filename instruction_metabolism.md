# Metabolism Stoichiometry Recovery — Training Parameter Optimization

**Reference**: See `neural-gnn/paper/main.pdf` for context.

## Goal

Find GNN training hyperparameters **and GNN code-level parameters** that recover the **stoichiometric matrix S** from metabolic concentration dynamics generated by PDE_M1 (100 metabolites, 256 reactions, max 4 metabolites per reaction).

**This is a fixed-regime exploration**: simulation parameters are FROZEN. GNN training parameters AND GNN code may be changed. There are NO block boundary simulation changes. **Code changes to GNN architecture/training are encouraged** if config-only sweeps plateau.

## Metabolism Regime Characteristics

The system models a metabolic network as a bipartite graph (metabolites <-> reactions) governed by stoichiometric kinetics:

```
dc_i/dt = sum_j S_ij * v_j(c)
```

- **c** = metabolite concentrations (100 metabolites)
- **S** = stoichiometric matrix (100 x 256), sparse, entries in {-2, -1, 0, +1, +2}
  - Negative entries: substrates (consumed by reaction)
  - Positive entries: products (produced by reaction)
  - Most entries are zero (each reaction involves 2-6 metabolites out of 100)
- **v_j(c)** = reaction rate for reaction j, computed as `k_j * softplus(rate_mlp(h_j))`
  - `k_j` = per-reaction rate constant, fixed, log-uniform in [0.01, 10]
  - `h_j` = aggregated message from substrate concentrations via msg_mlp

### Key Differences from Neural Signal Recovery

| Property | Signal (W recovery) | Metabolism (S recovery) |
| --- | --- | --- |
| Graph structure | Fixed edge_index, learnable W weights | Fixed bipartite graph (met <-> rxn), learnable S coefficients |
| Learnable weights | W (n_neurons x n_neurons), dense | sto_sub (n_sub_edges,) + sto_all (n_all_edges,), sparse |
| Weight semantics | Synaptic connectivity (arbitrary real) | Stoichiometric coefficients (small integers: -2,-1,+1,+2) |
| Dynamics | du/dt = phi(u, a) + W @ f(u, a) | dc/dt = S @ v(c) where v = k * softplus(rate_mlp(msg)) |
| Rate function | Edge MLP modulates W message | msg_mlp + rate_mlp compute reaction rates from substrate concentrations |
| Ground truth sparsity | Depends on regime (dense, sparse, low-rank) | Always sparse: ~3-6 non-zero entries per column (reaction) |
| Value distribution | Gaussian N(0, 1/sqrt(n)) | Discrete integers {-2, -1, +1, +2} |
| Number of parameters | n^2 = 10,000 (W) | ~600 substrate edges + ~1100 all edges |

### Stoichiometric Matrix Structure

- 256 reactions, each with 1-3 substrates and 1-3 products (coefficients 1 or 2)
- Total ~600 substrate edges (negative S entries) and ~500 product edges (positive S entries)
- S is very sparse: ~1100 non-zero entries out of 25,600 possible (100 x 256)
- Each column (reaction) has exactly 2-6 non-zero entries
- S has no special low-rank structure — it is a random sparse integer matrix

### Expected Challenges

- **Discrete target**: True S values are integers {-2,-1,+1,+2}, but the model learns continuous values. Post-training rounding may be needed for evaluation
- **Sign recovery**: Getting the correct sign (substrate vs product) for each edge is critical — wrong sign flips the contribution direction
- **Sparsity**: S is very sparse (~4% fill), so L1 regularization should help zero out incorrect entries
- **MLP compensation**: As with neural signal recovery, msg_mlp and rate_mlp can compensate for incorrect S coefficients
- **Rate constant interaction**: The model learns both S (stoichiometry) and the MLPs (rate functions). The rate constants k_j are also learnable — the model may find different k/S combinations that produce similar dynamics
- **Scale ambiguity**: scaling S by alpha and v by 1/alpha gives the same dx/dt. Regularization must break this symmetry

## Prior Knowledge (starting points)

These are hypotheses to validate, not fixed truths:
- `lr=1E-3` for the msg_mlp, rate_mlp, and log_k parameters
- `lr_embedding=1E-4` (not used in current model, but reserved)
- `lr_S=1E-3` (learning rate for sto_sub and sto_all; 0 means same as lr)
- `batch_size=8` is a reasonable starting point
- `data_augmentation_loop=1000` gives sufficient iterations per epoch
- L1 regularization on sto_all should promote correct sparsity pattern
- Two-phase training may help: first learn rate functions, then refine stoichiometry with L1

---

## Iteration Loop Structure

Each block = `n_iter_block` iterations. All blocks use the same simulation.
The prompt provides: `Block info: block {block_number}, iteration {iter_in_block}/{n_iter_block} within block`

## File Structure (CRITICAL)

You maintain **TWO** files:

### 1. Full Log (append-only record)

**File**: `{config}_analysis.md`

- Append every iteration's full log entry
- Append block summaries
- **Never read this file** — it's for human record only

### 2. Working Memory

**File**: `{config}_memory.md`

- **READ at start of each iteration**
- **UPDATE at end of each iteration**
- Contains: established principles + previous blocks summary + current block iterations
- Fixed size (~500 lines max)

---

## Iteration Workflow (Steps 1-5, every iteration)

### Step 1: Read Working Memory

Read `{config}_memory.md` to recall:

- Established principles
- Previous block findings
- Current block progress

### Step 2: Analyze Current Results

**Metrics from `analysis.log`:**

- `stoichiometry_R2`: **PRIMARY SCORE** — R2 of learned vs true stoichiometric coefficients (scatter plot: true S_ij vs learned S_ij)
- `final_loss`: final training prediction loss (MSE on dc/dt)
- `test_R2`: R2 between ground truth and rollout prediction of concentrations (from data_test_metabolism)
- `test_pearson`: Pearson correlation of concentration trajectories (from data_test_metabolism)

**Example analysis.log format:**

```
final_loss: 0.001234
stoichiometry_R2: 0.8500
test_R2: 0.7200
test_pearson: 0.8500
```

**Classification (based on stoichiometry_R2, the primary score):**

- **Converged**: stoichiometry_R2 > 0.9
- **Partial**: stoichiometry_R2 0.1-0.9
- **Failed**: stoichiometry_R2 < 0.1

**Degeneracy Detection (CRITICAL — check every iteration):**

Compute the **degeneracy gap** = `test_pearson - stoichiometry_R2`:

| test_pearson | stoichiometry_R2 | Degeneracy gap | Diagnosis |
|:---:|:---:|:---:|---|
| > 0.95 | > 0.9 | < 0.1 | **Healthy** — correct S |
| > 0.95 | 0.3-0.9 | 0.1-0.7 | **Degenerate** — MLP compensation |
| > 0.95 | < 0.3 | > 0.7 | **Severely degenerate** |
| < 0.5 | < 0.5 | ~0 | **Failed** — not degeneracy |

**When degeneracy gap > 0.3, DO NOT trust dynamics metrics as evidence of learning quality.**

**Stoichiometry Quality (check every iteration):**

When stoichiometry_R2 is partial (0.3-0.9), examine whether the issue is:
- Wrong sparsity pattern (non-zero entries in wrong locations)
- Right sparsity pattern but wrong magnitudes
- Wrong signs (substrates predicted as products or vice versa)
- Scale ambiguity (correct structure but wrong scale)

**Upper Confidence Bound (UCB) scores from `ucb_scores.txt`:**

- Provides computed UCB scores for all exploration nodes within a block
- At block boundaries, the UCB file will be empty (erased). When empty, use `parent=root`

### Step 3: Write Outputs

Append to Full Log (`{config}_analysis.md`) and **Current Block** sections of `{config}_memory.md`:

- In memory.md: Insert iteration log in "Iterations This Block" section (BEFORE "Emerging Observations")
- Update "Emerging Observations" at the END of the file

**Log Form:**

```
## Iter N: [converged/partial/failed]
Node: id=N, parent=P
Mode/Strategy: [exploit/explore/boundary/principle-test/degeneracy-break]
Config: seed=S, lr=X, lr_emb=Y, lr_S=Z, coeff_S_L1=W, coeff_S_L2=V, coeff_mass=M, batch_size=B, n_epochs=E, data_augmentation_loop=A
Metrics: stoichiometry_R2=C, test_R2=A, test_pearson=B, final_loss=E
Mutation: [param]: [old] -> [new]
Parent rule: [one line]
Observation: [one line]
Next: parent=P
```

**CRITICAL: The `Mutation:` line is parsed by the UCB tree builder. Always include the exact parameter change (e.g., `Mutation: lr_S: 1E-3 -> 5E-4`).**

### Step 4: Parent Selection Rule in UCB tree

Step A: Select parent node

- Read `ucb_scores.txt`
- If empty -> `parent=root`
- Otherwise -> select node with **highest UCB** as parent

**CRITICAL**: The `parent=P` in the Node line must be the **node ID** (integer), NOT "root" (unless UCB file is empty).

Step B: Choose strategy

| Condition | Strategy | Action |
|---|---|---|
| Default | **exploit** | Highest UCB node, conservative mutation |
| 3+ consecutive stoich_R2 >= 0.9 | **failure-probe** | Extreme parameter to find boundary |
| n_iter_block/4 consecutive successes | **explore** | Select outside recent chain |
| degeneracy gap > 0.3 for 3+ iters | **degeneracy-break** | Constrain MLPs, increase L1, reduce training duration |
| Same stoich_R2 plateau (+/-0.05) for 3+ iters | **forced-branch** | Select 2nd-highest UCB, switch param dimension |
| 4+ consecutive same-param mutations | **switch-dimension** | Change a different parameter |
| 2+ distant nodes with stoich_R2 > 0.9 | **recombine** | Merge best params from both nodes |
| stoich_R2 plateau after config sweeps | **code-modification** | Modify GNN code (see Step 5.2) |

### Step 5: Edit Config File

Edit config file for next iteration.

**CRITICAL: Config Parameter Constraints**

**DO NOT add new parameters to the `claude:` section.** Only these fields are allowed:
- `n_epochs`: int
- `data_augmentation_loop`: int
- `n_iter_block`: int
- `ucb_c`: float (0.5-3.0)

**DO NOT change `simulation:` parameters except `seed`.** The simulation regime is fixed for this exploration.

**Simulation Parameters (only seed is mutable):**

```yaml
training:
  seed: 42                         # changing seed generates a DIFFERENT stoichiometric matrix S
                                   # and different rate constants k_j
                                   # use different seeds to test robustness
```

**Training Parameters (the exploration space):**

Mutate ONE parameter at a time for causal understanding.

```yaml
training:
  learning_rate_start: 1.0E-3       # range: 1E-5 to 1E-2 (msg_mlp, rate_mlp, log_k)
  learning_rate_embedding_start: 1.0E-4  # range: 1E-5 to 1E-3
  learning_rate_S_start: 1.0E-3    # range: 1E-5 to 1E-2 (lr for sto_sub, sto_all; 0 = same as lr)

  coeff_S_L1: 0.0                  # L1 on sto_all (sparsity); range: 0 to 1E-1
  coeff_S_L2: 0.0                  # L2 on sto_all; range: 0 to 1E-1
  coeff_mass_conservation: 0.0     # penalize non-zero column sums of S (mass balance); range: 0 to 1.0

  batch_size: 8                    # values: 4, 8, 16, 32
  n_epochs: 10                     # range: 5-50
  data_augmentation_loop: 1000     # range: 100-2000

  # Two-phase training
  n_epochs_init: 2                 # epochs in phase 1 (no L1)
  first_coeff_L1: 0                # L1 during phase 1 (typically 0)
```

**Claude Exploration Parameters:**

```yaml
claude:
  ucb_c: 1.414    # UCB exploration constant (0.5-3.0)
```

### Step 5.2: Modify GNN Code (PREFERRED when config sweeps plateau)

**SIMPLE RULE: NEVER MODIFY CODE IF 'code' NOT IN TASK**

**When to modify code:**

- When config-level parameters produce identical results across sweeps (stoich_R2 plateau)
- When degeneracy gap > 0.3 persists despite regularization tuning
- When you have a specific architectural hypothesis to test
- NEVER modify code in first 4 iterations of a block (establish baseline first)

**Files you can modify:**

| File | Permission |
| --- | --- |
| `src/NeuralGraph/models/graph_trainer.py` | **ONLY modify `data_train_metabolism` function** |
| `src/NeuralGraph/models/Metabolism_Propagation.py` | Can modify if necessary |
| `src/NeuralGraph/models/MLP.py` | Can modify if necessary |
| `src/NeuralGraph/utils.py` | Can modify if necessary |

**Key model attributes (read-only reference):**

- `model.sto_sub` - Substrate stoichiometric coefficients `(n_sub_edges,)`, init: `randn * 0.1`
- `model.sto_all` - All stoichiometric coefficients `(n_all_edges,)`, init: `randn * 0.1`
- `model.msg_mlp` - Message MLP: `[concentration, |stoich|] -> message_vector (dim=16)`
- `model.rate_mlp` - Rate MLP: `aggregated_message -> scalar_rate (softplus)`
- `model.log_k` - Log10 rate constants `(n_rxn,)`, init: `uniform(-2, 1)`

**Priority code changes for metabolism regime (ordered by expected impact):**

1. **Stoichiometry initialization scale** — True S has entries in {-2,-1,+1,+2}. Current init is `randn * 0.1`, which may be too small. Try `randn * 1.0` to start closer to the true scale
2. **L1 regularization on sto_all** — Promote sparsity in the learned stoichiometric matrix. Add soft-thresholding: `model.sto_all.data = sign(S) * clamp(|S| - threshold, min=0)`
3. **Integer rounding loss** — Add a penalty that pushes sto_all values toward the nearest integer: `round_loss = ((sto_all - round(sto_all))**2).mean()`
4. **Sign constraint** — sto_sub should always be positive (absolute values). Add `sto_sub = |sto_sub|` in forward pass (already done via `.abs()`)
5. **MLP capacity reduction** — Smaller msg_mlp/rate_mlp (hidden_dim: 32->16) forces S to carry more signal
6. **Gradient clipping on sto_all** — Prevent large gradient updates that destabilize stoichiometry learning
7. **Separate learning rates** — Use different lr for sto_sub/sto_all vs msg_mlp/rate_mlp vs log_k
8. **Loss weighting** — Weight the MSE loss per metabolite by inverse participation count (metabolites in more reactions have more signal)

**Safety rules (CRITICAL):**

1. **Make minimal changes** — edit only what's necessary
2. **Test in isolation first** — don't combine code + config changes in the same iteration
3. **Document thoroughly** — explain WHY in mutation log
4. **One change at a time** — never modify multiple functions simultaneously
5. **Preserve interfaces** — don't change function signatures

**Logging Code Modifications:**

```
## Iter N: [converged/partial/failed]
Node: id=N, parent=P
Mode/Strategy: code-modification
Config: [unchanged from parent, or specify if also changed]
CODE MODIFICATION:
  File: src/NeuralGraph/models/graph_trainer.py
  Function: data_train_metabolism
  Change: [what was changed]
  Hypothesis: [why this should help stoichiometry recovery]
Metrics: stoichiometry_R2=C, test_R2=A, test_pearson=B, final_loss=E
Mutation: [code] data_train_metabolism: [short description]
Parent rule: [one line]
Observation: [compare to parent — did code change help?]
Next: parent=P
```

**NEVER:**

- Modify GNN_LLM.py or GNN_LLM_parallel.py (breaks the experiment loop)
- Change function signatures (breaks compatibility)
- Add dependencies requiring new pip packages
- Make multiple simultaneous code changes (can't isolate causality)
- Modify code just to "try something" without a specific hypothesis

**ALWAYS:**

- Explain the hypothesis motivating the code change
- Compare directly to parent iteration (same config, code-only diff)
- Document exactly what changed (file, function, what was added/removed)
- Consider reverting a code change if it doesn't help (git checkout the file)

---

## Block Workflow (Steps 1-3, every end of block)

Triggered when `iter_in_block == n_iter_block`

### STEP 1: Edit Instructions (this file)

You **MUST** use the Edit tool to add/modify parent selection rules.

**Evaluate:**
- Branching rate < 20% -> ADD exploration rule
- Improvement rate < 30% -> INCREASE exploitation
- Same stoich_R2 plateau for 3+ iters -> ADD forced branching
- > 4 consecutive same-param mutations -> ADD switch-dimension rule

### STEP 2: Choose Next Block Focus

Since simulation is fixed, blocks explore different **training parameter subspaces**:
- Block 1: lr and L1 sweep (central parameters for stoichiometry recovery)
- Block 2: L1 calibration — find optimal sparsity pressure for S
- Block 3: Two-phase training / MLP capacity interaction
- Block 4+: Refine based on findings

**At block boundaries, choose which parameter subspace to explore next.**

### STEP 3: Update Working Memory

Update `{config}_memory.md`:

- Update Knowledge Base with confirmed principles
- Replace Previous Block Summary with **short summary** (2-3 lines)
- Clear "Iterations This Block" section
- Write hypothesis for next block

---

# Working Memory Structure

## Knowledge Base (accumulated across all blocks)

### Best Configurations Found

| Blk | lr | lr_S | coeff_S_L1 | batch | n_epochs | aug_loop | stoich_R2 | test_R2 | Finding |
| --- | -- | ---- | ---------- | ----- | -------- | -------- | --------- | ------- | ------- |
| 1   | 1E-3 | 1E-3 | 0 | 8 | 10 | 1000 | ? | ? | baseline |

### Established Principles

[Confirmed patterns — require 3+ supporting iterations]

### Open Questions

[Patterns needing more testing, contradictions]

---

## Previous Block Summary (Block N-1)

[Short summary only]

---

## Current Block (Block N)

### Block Info

Focus: [which parameter subspace]

### Hypothesis

[Prediction for this block]

### Iterations This Block

[Current block iterations — cleared at block boundary]

### Emerging Observations

[Running notes]
**CRITICAL: This section must ALWAYS be at the END of memory file.**

---

## Background

### Metabolic Network Architecture (Metabolism_Propagation)

The training model mirrors the PDE_M1 generator but with **learnable stoichiometric coefficients**:

```
dc_i/dt = sum_j  sto_all[e] * v[rxn_all[e]]     for all edges e where met_all[e] == i
```

Where:
- `sto_all` (nn.Parameter): signed stoichiometric coefficients for all edges
- `sto_sub` (nn.Parameter): absolute stoichiometric coefficients for substrate edges
- `v[j] = k_j * softplus(rate_mlp(h_j))`: non-negative reaction rate
- `h_j = sum over substrate edges of reaction j: msg_mlp([concentration, |sto_sub|])`
- `k_j = 10^(log_k[j])`: per-reaction rate constant (learnable)

### Bipartite Graph Structure (fixed buffers)

```
met_sub[e], rxn_sub[e]  — substrate edges (metabolite -> reaction)
met_all[e], rxn_all[e]  — all edges (substrate + product)
```

The graph structure is fixed (which metabolites participate in which reactions). Only the stoichiometric coefficients on edges are learnable.

### Generator (PDE_M1) — Ground Truth

```
dc_i/dt = sum_j  S_true[i,j] * v_j(c)
```

- S_true: fixed stoichiometric matrix (100 x 256), entries in {-2, -1, 0, +1, +2}
- v_j: reaction rates from random MLPs with fixed rate constants k_j
- No external input (PDE_M1 = pure stoichiometric kinetics)

### Training Loss

```
L = L_pred + coeff_S_L1 * ||sto_all||_1 + coeff_S_L2 * ||sto_all||_2 + coeff_mass * sum_j (sum_i S_ij)^2
```

- `L_pred`: MSE between predicted dc/dt and true dc/dt
- `coeff_S_L1`: L1 on sto_all (promotes sparsity = correct zero pattern in S)
- `coeff_S_L2`: L2 on sto_all (weight decay on stoichiometric coefficients)
- `coeff_mass_conservation`: penalizes non-zero column sums of S (mass balance per reaction)

### Metabolism-Specific Considerations

- **No connectivity matrix W**: Unlike signal recovery, there is no W matrix. The learnable "weights" are the stoichiometric coefficients sto_sub and sto_all
- **Concentration dynamics, not neural activity**: The state variable is metabolite concentration c (always non-negative), not membrane potential u
- **Integer target**: Ground truth S has discrete integer entries, unlike W which is continuous Gaussian
- **Very sparse target**: S has ~4% fill factor, much sparser than typical W matrices
- **Rate function coupling**: The msg_mlp and rate_mlp jointly determine reaction rates. Incorrect stoichiometry can be partially compensated by different rate functions (degeneracy)
