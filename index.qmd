---
title: "Experiment-LLM-Memory"
subtitle: "Closed-loop scientific exploration in neural connectivity inference"
author: "Cédric Allier, Stephan Saalfeld"
date: "2026-02-03"
---

## Introduction

We previously showed that graph neural networks can recover circuit structure and signaling functions of neural assemblies from activity data alone. However, this inverse problem is known to be ill-posed under certain conditions. For instance, when neural activity is low-rank, many different circuits can generate the same neuron traces. It remains an open question whether graph neural networks can recover connectivity in general, or only for specific classes of neural assemblies.

To address this question, we extend our graph neural network framework with a large language model. In this new setup, the role of the graph neural network framework is to perform experiments and deliver quantified results. The role of the large language model is to interpret, compare and finally compress the experiment results into structured memory. On the basis thereof, the large language model is next prompted to mutate selectively the neural dynamics configuration and/or the graph neural network training scheme.

We show that these sequential interactions between experimentation, large language model and long-term memory, lead progressively to a scientific tool. Testable hypotheses are drawn, repeatable experiments are conducted to validate or falsify them, and ultimately causal understanding emerges. Importantly, the **closed-loop scientific reasoning results from the interactions between the three components** rather than residing solely within the large language model.

---

## The Exploration Loop

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[Experiment] --> B[LLM]
    B --> A

    B --> C[(Memory)]
    C --> B

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#f3e5f5
```

The framework implements a **closed-loop exploration engine** composed of three interacting components:

1. **Experiment**
   A physics-based simulator generates neural activity. A message-passing GNN learns to predict activity derivatives while jointly recovering the connectivity matrix W. **4 parallel slots** run simultaneously per batch via UCB tree search.

2. **LLM**
   The LLM interprets results in context of accumulated memory, performs scientific operations (identify regimes, detect convergence, generate hypotheses), and selects the next intervention via UCB tree search.

3. **Memory**
   Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error.

---

## [Results](results.qmd)

Connectivity recovery succeeds for dense chaotic networks up to n=600 (100% convergence at 30k frames), low-gain networks (g=3, n=200) rescued by 30k frames, and g=2 partially rescued (42% convergence at 30k with inverse lr_W). The dominant lever is n_frames: tripling from 10k to 30k transforms n=300 from 25% to 100% convergence, n=600 from 0% to 100%, and g=3/n=200 from 0% to 100%. Three structural limits identified: subcritical spectral radius (sparse 50%, conn~0.49), partial connectivity ceiling (fill<100%, conn ≈ filling_factor confirmed at 50/80/90/100%), and g=1 fixed-point collapse (eff_rank drops to 1 at 30k --- hardest regime). n=1000 mapped at 30k (conn=0.745, needs ~100k frames). ~80 principles established across 28 regimes over 336 iterations.

[![](assets/landscape_quadrant.png){.lightbox}](results.qmd)

---

## [Epistemics](epistemic-analysis.qmd)

400+ reasoning events classified across 336 iterations and 28 blocks into ten formal epistemic modes (induction, deduction, falsification, analogy, boundary probing, etc.), connected by 150+ causal edges. The system achieves 74% deduction accuracy (well above chance), 70% cross-regime analogy transfer success, and 100% falsification-to-refinement rate. ~80 principles established with confidence 45--100%.

Six distinct reasoning phases emerge: (1) boundary probing and induction dominate early as the system maps each new regime, (2) deduction and analogy grow as accumulated principles enable cross-regime predictions, (3) falsification and constraint recognition widen when structurally hard regimes are encountered, (4) analogy drives scaling explorations as n_frames dominance overturns multiple principles (blocks 15--16), (5) constraint recognition peaks as the system maps structural limits --- subcritical spectral radius (block 17), conn_ceiling at filling_factor (blocks 22--24) --- while confirming that gain is a solvable difficulty axis (blocks 19--21), and (6) regime recognition identifies the gain--eff_rank critical transition (blocks 25--28), identifying fixed-point collapse (g=1) as a new unsolvable axis and inverse lr_W at g=2.

[![](assets/Sankey.png){.lightbox}](epistemic-analysis.qmd)

---

## [Exploration](exploration-gallery.qmd)

UCB tree search guides the exploration across 336 iterations and 28 blocks, processing 4 parallel slots per batch. At each step, the LLM selects parent configurations to mutate using an Upper Confidence Bound strategy that balances exploitation of high-performing branches with exploration of under-visited regions. The tree grows through 28 regime blocks --- from chaotic baselines (n=100) through sparse connectivity, scale challenges (n=200, 300, 600, 1000), heterogeneous networks, recurrent training, n_frames scaling experiments, gain reduction (g=1--3), and partial connectivity (fill=80--90%).

Each iteration produces a connectivity scatter plot, kinograph (dynamics visualization), and MLP embedding analysis. The UCB tree snapshots show how the search progressively narrows: early blocks fan out widely as the system maps parameter boundaries, middle blocks concentrate on promising subtrees while pruning failed branches, and late blocks show rapid convergence as n_frames dominance reduces the effective search dimension. Blocks 17--23 show flat UCB landscapes when structural limits dominate (sparse, fill<100%) versus rapid convergence when n_frames rescues a regime (g=3/n=200). Blocks 24--28 reveal the gain--eff_rank critical transition: g=1 produces flat landscapes (no parameter matters), while g=2 shows a narrow ridge requiring inverse lr_W.

[![](log/Claude_exploration/instruction_signal_landscape_parallel/exploration_tree/ucb_tree_iter_264.png){.lightbox}](exploration-gallery.qmd)

---

## Case Studies

### [Low-Rank Connectivity](case-low-rank.qmd)

Can GNN recover connectivity from rank-20 dynamics (eff_rank ~12)? The landscape exploration found a breakthrough recipe (lr_W=3E-3, L1=1E-6 → conn_R^2^=0.993, test_R^2^=0.996) in 12 iterations. A dedicated exploration loop tested 7 random seeds and established a universal recipe template (lr=1E-4, edge_diff=10000, batch=8, n_epochs_init=2) with only lr_W and L1 requiring per-seed tuning. All 7 seeds reach test_R^2^≥0.985 when optimally configured.

### [Sparse Connectivity](case-sparse.qmd)

Can code-level GNN modifications break the conn_R^2^=0.489 ceiling in the sparse regime? The dedicated exploration tested 4 code modifications (W init scale, proximal L1 soft-thresholding, MLP capacity reduction, gradient clipping) across 24 iterations. All failed: the architectural degeneracy is fundamental --- the `lin_phi` bypass pathway compensates perfectly, leaving W unconstrained regardless of training configuration or local code changes.

---

## References

1. Stern, M., et al. (2023). Graph neural networks uncover structure and function underlying the activity of neural assemblies.

2. Romera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. *Nature*.

3. Novikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration.
