---
title: "Experiment-LLM-Memory"
subtitle: "Closed-loop scientific exploration in neural connectivity inference"
author: "Cédric Allier, Stephan Saalfeld"
date: "2026-01-31"
---

## Introduction

We previously showed that graph neural networks can recover circuit structure and signaling functions of neural assemblies from activity data alone. However, this inverse problem is known to be ill-posed under certain conditions. For instance, when neural activity is low-rank, many different circuits can generate the same neuron traces. It remains an open question whether graph neural networks can recover connectivity in general, or only for specific classes of neural assemblies.

To address this question, we extend our graph neural network framework with a large language model. In this new setup, the role of the graph neural network framework is to perform experiments and deliver quantified results. The role of the large language model is to interpret, compare and finally compress the experiment results into structured memory. On the basis thereof, the large language model is next prompted to mutate selectively the neural dynamics configuration and/or the graph neural network training scheme.

We show that these sequential interactions between experimentation, large language model and long-term memory, lead progressively to a scientific tool. Testable hypotheses are drawn, repeatable experiments are conducted to validate or falsify them, and ultimately causal understanding emerges. Importantly, the **closed-loop scientific reasoning results from the interactions between the three components** rather than residing solely within the large language model.

---

## The Exploration Loop

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[Experiment] --> B[LLM]
    B --> A

    B --> C[(Memory)]
    C --> B

    style A fill:#e1f5fe
    style B fill:#fff3e0
    style C fill:#f3e5f5
```

The framework implements a **closed-loop exploration engine** composed of three interacting components:

1. **Experiment**
   A physics-based simulator generates neural activity following the Stern et al. (2023) model. A message-passing GNN learns to predict activity derivatives while jointly recovering the connectivity matrix W. **4 parallel slots** run simultaneously per batch via UCB tree search.

2. **LLM**
   The LLM interprets results in context of accumulated memory, performs scientific operations (identify regimes, detect convergence, generate hypotheses), and selects the next intervention via UCB tree search.

3. **Memory**
   Observations, failed attempts, and validated principles are written into explicit long-term memory. This memory persists across experimental blocks, enabling cumulative understanding rather than episodic trial-and-error.

---

## [Results](results.qmd)

Connectivity recovery succeeds for dense chaotic networks up to n=600 (100% convergence at 30k frames) and low-gain networks (g=3, n=200) rescued by 30k frames. The dominant lever is n_frames: tripling from 10k to 30k transforms n=300 from 25% to 100% convergence, n=600 from 0% to 100%, and g=3/n=200 from 0% to 100%, with best conn_R^2^ = 0.996. Two structural limits identified: subcritical spectral radius (sparse 50%, conn stuck at ~0.49) and partial connectivity ceiling (fill=80%, conn locked at ~0.80 = filling_factor). n=1000 mapped at 30k (conn=0.745, needs ~100k frames). 65 principles established across 23 regimes over 276 iterations.

[![](assets/landscape_quadrant.png){.lightbox}](results.qmd)

---

## [Epistemics](epistemic-analysis.qmd)

330+ reasoning events classified across 276 iterations and 23 blocks into ten formal epistemic modes (induction, deduction, falsification, analogy, boundary probing, etc.), connected by 125+ causal edges. The system achieves 74% deduction accuracy (well above chance), 70% cross-regime analogy transfer success, and 100% falsification-to-refinement rate. 65 principles discovered with confidence 45--100%.

Five distinct reasoning phases emerge: (1) boundary probing and induction dominate early as the system maps each new regime, (2) deduction and analogy grow as accumulated principles enable cross-regime predictions, (3) falsification and constraint recognition widen when structurally hard regimes are encountered, (4) analogy drives scaling discoveries as n_frames dominance overturns multiple principles (blocks 15--16), and (5) constraint recognition peaks as the system maps structural limits --- subcritical spectral radius (block 17), conn_ceiling at filling_factor (blocks 22--23) --- while confirming that gain is a solvable difficulty axis (blocks 19--21).

[![](assets/Sankey.png){.lightbox}](epistemic-analysis.qmd)

---

## [Exploration](exploration-gallery.qmd)

UCB tree search guides the exploration across 276 iterations and 23 blocks, processing 4 parallel slots per batch. At each step, the LLM selects parent configurations to mutate using an Upper Confidence Bound strategy that balances exploitation of high-performing branches with exploration of under-visited regions. The tree grows through 23 regime blocks --- from chaotic baselines (n=100) through sparse connectivity, scale challenges (n=200, 300, 600, 1000), heterogeneous networks, recurrent training, n_frames scaling experiments, gain reduction (g=3), and partial connectivity (fill=80%).

Each iteration produces a connectivity scatter plot, kinograph (dynamics visualization), and MLP embedding analysis. The UCB tree snapshots show how the search progressively narrows: early blocks fan out widely as the system maps parameter boundaries, middle blocks concentrate on promising subtrees while pruning failed branches, and late blocks show rapid convergence as n_frames dominance reduces the effective search dimension. Blocks 17--23 show flat UCB landscapes when structural limits dominate (sparse, fill=80%) versus rapid convergence when n_frames rescues a regime (g=3/n=200).

[![](log/Claude_exploration/instruction_signal_landscape_parallel/exploration_tree/ucb_tree_iter_264.png){.lightbox}](exploration-gallery.qmd)

---

## Case Studies

### [Low-Rank Connectivity](case-low-rank.qmd)

Can GNN recover connectivity from rank-20 dynamics (eff_rank ~12)? The landscape exploration found a breakthrough recipe (lr_W=3E-3, L1=1E-6 → conn_R^2^=0.993, test_R^2^=0.996) in 12 iterations. A dedicated exploration loop now probes robustness across seeds, two-phase training, and edge differentiation priors.

---

## References

1. Stern, M., et al. (2023). Graph neural networks uncover structure and function underlying the activity of neural assemblies.

2. Romera-Paredes, B., et al. (2024). Mathematical discoveries from program search with large language models. *Nature*.

3. Novikov, A., et al. (2025). AlphaEvolve: A coding agent for scientific and algorithmic exploration.
